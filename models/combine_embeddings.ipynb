{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import re\n",
    "\n",
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 128\n",
    "TRAIN_VAL_SIZE = 0.2\n",
    "VAL_TEST_SIZE = 0.5\n",
    "EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predictions):\n",
    "  f1 = f1_score(true_labels, predictions, average=\"macro\")\n",
    "  precision = precision_score(true_labels, predictions, average=\"macro\")\n",
    "  recall = recall_score(true_labels, predictions, average=\"macro\")\n",
    "  accuracy = accuracy_score(true_labels,predictions)\n",
    "  return f1, precision, recall, accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_df = pd.read_csv(\"../datasets/bbc/bbc-text.csv\")\n",
    "bbc_df[\"category\"] = pd.Categorical(bbc_df['category']).codes\n",
    "labels = bbc_df[\"category\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=re.sub('<br \\/>','',text) \n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text) \n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A female vampire kills young women and paints ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Personally I think this show looks pretty chea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I grew up watching Inspector Gadget It was and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is awful Im SORRY I bought this to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a great example of a good dumb movie N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>After watching this on the MST3K episode I hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Upon completing this infernal piece of trash a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>Maybe Im biased because the F16 is my favorite...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>The Best Movie of the 90s The Welsh Trainspott...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>This was a excellent back when it came out It ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0     A female vampire kills young women and paints ...          0\n",
       "1     Personally I think this show looks pretty chea...          0\n",
       "2     I grew up watching Inspector Gadget It was and...          0\n",
       "3     This movie is awful Im SORRY I bought this to ...          0\n",
       "4     This is a great example of a good dumb movie N...          1\n",
       "...                                                 ...        ...\n",
       "4995  After watching this on the MST3K episode I hav...          0\n",
       "4996  Upon completing this infernal piece of trash a...          0\n",
       "4997  Maybe Im biased because the F16 is my favorite...          1\n",
       "4998  The Best Movie of the 90s The Welsh Trainspott...          0\n",
       "4999  This was a excellent back when it came out It ...          1\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = pd.read_csv(\"../datasets/movies/sampled.csv\")\n",
    "movies_df[\"sentiment\"] = pd.Categorical(movies_df['sentiment']).codes\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df = pd.read_csv(\"../datasets/twitter_sampled.csv\")\n",
    "labels = twitter_df[\"label\"].unique()\n",
    "twitter_df[\"label\"] = twitter_df[\"label\"].apply(lambda x: 1 if x==4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>why and she screaming ahaha this song is funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the_trini_bajan work as usual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>desi_f pack me in your luggage I wanna go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>elm8 Thanks  I enjoy talking to you too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>watchin the season finale of The Office lets h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>0</td>\n",
       "      <td>So sleepy this morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>0</td>\n",
       "      <td>bakespace do you archive your newsletters some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>1</td>\n",
       "      <td>santyadh hope that will soon change though  bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>0</td>\n",
       "      <td>I think I should do my homework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>0</td>\n",
       "      <td>This is officially the only day since starting...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         1    why and she screaming ahaha this song is funny \n",
       "1         0                     the_trini_bajan work as usual \n",
       "2         0         desi_f pack me in your luggage I wanna go \n",
       "3         1            elm8 Thanks  I enjoy talking to you too\n",
       "4         1  watchin the season finale of The Office lets h...\n",
       "...     ...                                                ...\n",
       "4795      0                           So sleepy this morning  \n",
       "4796      0  bakespace do you archive your newsletters some...\n",
       "4797      1  santyadh hope that will soon change though  bo...\n",
       "4798      0                   I think I should do my homework \n",
       "4799      0  This is officially the only day since starting...\n",
       "\n",
       "[4800 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = bert_tokenizer.tokenize(marked_text, truncation=True)\n",
    "    indexed_tokens = bert_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    # convert inputs to tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_embeddings(tokens_tensor, segments_tensor):\n",
    "    with torch.no_grad():\n",
    "      # obtain hidden states\n",
    "      outputs = bert_model(tokens_tensor, segments_tensor)\n",
    "      hidden_states = outputs[2]\n",
    "    # concatenate the tensors for all layers\n",
    "    # use \"stack\" to create new dimension in tensor\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    # remove dimension 1, the \"batches\"\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    # swap dimensions 0 and 1 so we can loop over tokens\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    # intialized list to store embeddings\n",
    "    token_vecs_sum = []\n",
    "    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
    "    # where Y is the number of tokens in the sentence\n",
    "    # loop over tokens in sentence\n",
    "    for token in token_embeddings:\n",
    "    # \"token\" is a [12 x 768] tensor\n",
    "    # sum the vectors from the last four layers\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'is', 'a', 'test', 'sentence', '[SEP]']\n",
      "[tensor([-1.2918e+00, -3.4288e-01, -8.7811e-01, -2.8823e+00, -1.6881e+00,\n",
      "        -1.9190e+00,  1.4351e+00,  1.0858e+00,  1.2602e+00, -2.0788e+00,\n",
      "        -1.1483e+00, -8.2739e-01, -1.2627e+00,  6.1478e-01, -7.9215e-01,\n",
      "        -8.9600e-01, -7.8040e-01,  8.7334e-01,  4.3941e-01, -2.4041e+00,\n",
      "        -7.1802e-01, -8.9348e-01, -2.0710e+00, -2.0920e+00,  9.2590e-01,\n",
      "        -1.3476e+00, -9.0515e-01, -2.8579e-01, -1.9834e-02,  1.5233e+00,\n",
      "         6.3897e-01,  1.8940e+00, -1.7642e+00,  1.0027e+00,  2.3684e+00,\n",
      "         9.7974e-01, -3.8899e-01, -1.5129e-01, -1.5964e-01,  1.6982e+00,\n",
      "        -1.7430e+00, -1.5724e-01,  1.0982e+00, -1.0403e+00, -1.3274e+00,\n",
      "        -8.5357e-01, -1.6697e+01,  3.5604e-01,  3.7055e-04, -3.6944e+00,\n",
      "        -1.2191e+00,  6.5086e-01, -3.4823e-02,  3.2367e+00,  1.4870e+00,\n",
      "         4.6398e-01, -1.9974e+00, -6.2199e-01, -6.4532e-01, -4.1324e-01,\n",
      "        -1.0280e+00,  2.4617e-01, -4.5789e-01,  6.0787e-02,  3.3384e-01,\n",
      "         1.2350e+00,  1.5518e+00,  5.8902e-01, -2.9399e+00,  2.4344e+00,\n",
      "        -3.0437e+00,  7.9194e-01, -2.7052e-01, -5.0633e-01,  1.0378e+00,\n",
      "         6.5455e-01, -1.5197e+00,  1.3118e+00, -1.4682e+00, -2.1106e-01,\n",
      "        -1.7218e+00,  1.7916e+00,  1.2696e+00, -1.1076e+00,  7.1338e-01,\n",
      "         1.3173e+00, -5.2266e-01, -2.9520e+00, -7.4855e-01,  2.8089e+00,\n",
      "        -7.4851e-01,  1.7728e+00, -5.2966e-01, -1.5747e-01,  1.3425e+00,\n",
      "        -3.0998e-01,  1.2473e-01, -9.2398e-02,  2.2684e+00,  5.3672e-01,\n",
      "        -5.6894e-02, -1.2097e+00,  4.3971e-01, -1.9688e+00, -7.7044e-01,\n",
      "        -3.2619e-01, -1.3725e-01,  2.1912e-01, -3.9628e-01, -2.4538e+00,\n",
      "         2.0502e+00,  9.7011e-01,  1.5081e-01, -2.5378e+00, -8.4731e-01,\n",
      "         1.9006e+00,  5.8409e-01,  3.9152e-01,  1.6438e+00, -7.1888e-01,\n",
      "        -1.7413e+00, -6.1832e-01,  4.2013e-01, -7.6815e-01,  2.8122e+00,\n",
      "         1.0026e+00,  6.0332e-01,  4.9595e-01,  1.7648e+00,  1.0965e+00,\n",
      "         1.1606e+00,  2.0011e+00,  1.7726e+00,  4.2346e-01, -7.7864e-01,\n",
      "         1.3931e+00,  9.1042e-01, -1.6069e+00, -2.8353e+00,  1.1539e+00,\n",
      "        -7.9703e-01, -6.1462e-01, -5.6576e+00,  1.5732e+00,  2.3263e+00,\n",
      "         2.7359e-01, -2.1317e+00,  2.3394e-01,  1.5655e+00, -4.9859e-01,\n",
      "         1.6887e-01,  1.4626e+00, -8.2933e-01,  1.4604e+00, -1.6411e+00,\n",
      "        -1.7106e+00, -1.7536e-01, -8.2347e-01,  1.3616e+00,  1.7469e+00,\n",
      "        -7.7550e-03, -2.4189e-01,  1.6349e+00, -1.9707e+00,  2.3370e-01,\n",
      "        -1.8550e+00,  6.0694e-01,  2.2373e+00,  1.5285e+00, -2.0240e+00,\n",
      "        -1.4478e+00, -8.7840e-01,  3.3858e+00, -1.0059e+00,  2.4675e-01,\n",
      "         1.5342e-02,  9.2238e-01,  1.3832e+00,  1.2949e+00,  6.5506e-02,\n",
      "        -6.7776e-01,  1.5209e+00,  6.5193e-01, -5.4274e-01,  1.0145e+00,\n",
      "         1.2075e-01,  1.5550e+00, -1.8692e+00,  1.1066e-01,  3.4356e-01,\n",
      "        -1.0059e+00, -1.1218e-01,  6.8143e-02, -3.5269e+00,  7.7275e-01,\n",
      "         1.1512e+00, -4.5668e-02, -1.8009e+00,  4.6832e-01, -2.1142e-01,\n",
      "        -1.3891e-01,  1.5556e+00, -2.1171e+00, -1.4357e+00,  1.6914e-01,\n",
      "         1.1180e+01,  5.8954e-01,  3.1282e-01,  2.2196e+00,  1.9608e+00,\n",
      "        -1.1020e+00,  2.3297e+00, -1.8150e+00, -2.1566e+00,  1.1129e+00,\n",
      "         5.4621e-01,  1.4985e+00, -6.6165e-01,  5.5496e-01,  1.0745e-01,\n",
      "         8.2686e-01,  1.5622e+00, -1.6013e-01,  5.7820e-01, -2.4274e+00,\n",
      "         1.5992e+00,  1.9922e+00,  1.7128e+00,  2.3472e+00, -7.6204e+00,\n",
      "         9.9476e-01,  1.7552e-01, -5.9132e-01,  7.0236e-01, -1.0929e+00,\n",
      "        -1.3250e+00,  1.7810e+00,  8.2735e-01,  8.1832e-01, -2.1537e+00,\n",
      "        -1.6555e+00,  3.1827e+00,  1.4160e+00,  2.2879e-01,  1.9722e+00,\n",
      "         2.2927e-01,  1.4571e+00, -1.1452e+00, -3.2789e-01,  2.2248e-02,\n",
      "         1.1596e+00, -1.0889e+00, -7.7496e-01, -9.3389e-01, -1.6650e+00,\n",
      "         1.2715e+00, -2.3831e-01, -8.7617e-01, -5.8312e-02,  3.2815e-01,\n",
      "        -1.4165e+00,  5.6792e-01, -2.7412e-01,  2.4174e+00, -1.9083e+00,\n",
      "        -5.3577e-02,  3.3388e-01, -4.1112e-01,  2.2624e+00, -9.9211e-01,\n",
      "        -1.5667e+00, -2.1924e+00, -3.5161e+00, -4.2351e+00,  6.4711e-01,\n",
      "        -1.5344e+00,  5.3681e-01,  2.5497e+00,  4.3447e-01,  4.8876e-01,\n",
      "         1.3655e-01, -7.7736e-02, -2.2507e+00,  6.5228e-01,  3.4838e+00,\n",
      "         8.0817e-01,  6.4076e-02, -3.8215e+00,  2.5662e+00,  5.5542e-02,\n",
      "        -2.2812e+00, -2.5924e-01, -1.9005e+00, -8.5655e-01,  4.6050e+00,\n",
      "        -2.9715e-01,  5.5262e-02, -2.8569e-01,  5.6351e-02, -3.5969e+00,\n",
      "        -2.2402e+00, -3.8567e-01,  1.2310e+00, -1.8386e+00, -7.9356e-01,\n",
      "         5.7240e-01, -1.4287e+00, -8.3156e-01, -3.7516e+01,  1.0703e+00,\n",
      "        -5.1809e-01, -1.1575e+00, -9.9850e-01, -1.5647e+00, -2.4351e-02,\n",
      "         1.7678e-01, -3.5858e-01, -1.9571e-01, -1.7160e-01, -1.4758e+00,\n",
      "        -1.6089e+00,  8.3426e-01,  2.3690e+00, -1.4507e+00,  1.7020e+00,\n",
      "        -5.2815e-01,  1.2905e-01, -6.2173e-01, -2.6188e+00,  5.2853e-01,\n",
      "        -2.9059e-01, -1.9424e-01, -6.1346e-01, -1.4068e+00, -2.5001e+00,\n",
      "         1.0584e-01,  1.9309e+00, -3.2726e+00, -1.1925e-01, -2.1048e+00,\n",
      "        -1.4788e+00, -1.1531e+00, -2.7819e+00, -9.4065e-01, -7.9192e-01,\n",
      "        -1.5053e-01,  4.1032e+00, -2.5625e-01, -1.8661e+00,  2.0519e+00,\n",
      "         1.0786e+00,  6.1957e-01,  9.9338e-01, -8.5876e-01,  1.1762e+00,\n",
      "         4.5659e-01,  7.2014e-01,  1.1065e+00, -3.4282e-01, -9.1350e-01,\n",
      "         5.8026e+00, -1.3400e+00,  1.9382e+00, -1.0448e+00,  1.5859e+00,\n",
      "         1.3470e+00,  1.1252e+00, -7.9089e-01,  3.9390e+00, -6.6025e-01,\n",
      "         2.1212e+00, -3.9573e-01,  9.9183e-01, -1.7427e+00,  2.1881e+00,\n",
      "        -1.0291e-02, -6.5857e-01, -6.1655e-01,  8.9440e-01,  1.6863e+00,\n",
      "         2.5584e-01, -1.1810e+01, -1.4445e+00, -1.8567e+00, -1.2890e+00,\n",
      "         7.8700e-01,  5.5262e-01, -6.7194e-01, -2.0062e+00, -1.3879e+00,\n",
      "        -1.7146e+00, -4.7569e-02, -1.1261e+00, -1.8542e-01, -8.2973e-01,\n",
      "        -3.3365e-03, -8.7504e-01,  7.3513e-01,  4.1819e-01,  6.1618e-01,\n",
      "        -2.8723e-01,  1.5843e+00, -1.3834e+00,  4.7334e-01,  1.2054e+00,\n",
      "        -2.9334e+00,  5.3509e-01, -8.4638e-01, -1.1190e+00, -1.8331e+00,\n",
      "        -8.7534e-01,  1.3952e+00, -1.4298e+00, -5.4354e-01, -2.0211e+00,\n",
      "         3.7039e+00, -3.8795e-01,  2.5566e+00,  7.0415e-01,  1.1682e+00,\n",
      "         1.7503e+00,  1.2635e+00,  2.6687e+00,  7.2056e-01, -1.7179e+00,\n",
      "         2.0074e+00,  6.5185e-03, -1.3707e-01,  1.0310e+00, -1.5604e+00,\n",
      "         9.5891e-01, -3.2584e-01, -1.2694e+00, -5.8963e-02, -8.0385e-01,\n",
      "        -1.6759e+00, -1.8701e+00, -5.3190e-01,  7.9152e-01,  2.4240e+00,\n",
      "        -1.3134e+00, -9.9245e-01, -4.9892e-01, -7.4883e-01, -6.2573e-01,\n",
      "         4.0281e+00,  1.9696e+00,  1.6667e+00,  3.4800e+00,  1.3321e+00,\n",
      "        -2.3432e+00,  2.3732e+00, -5.9901e-01,  1.5387e+00, -9.2402e-01,\n",
      "        -4.5496e-01,  1.0792e+00,  2.5873e+00,  1.1555e+00, -1.9139e+00,\n",
      "         1.8187e+00, -1.9031e-01, -5.4292e-01, -4.0053e-01, -1.2389e+00,\n",
      "        -1.9420e+00, -3.6453e-01, -4.5418e-01,  2.2602e+00, -2.4941e-02,\n",
      "        -9.9482e+00, -9.1998e-02,  1.8631e+00, -5.1909e-01, -3.1904e-01,\n",
      "         8.7844e-01, -4.0703e-01,  1.7810e+00, -8.2321e-01, -9.3219e-01,\n",
      "         1.0853e+00,  1.3083e+00, -4.9294e-01, -1.0941e+00,  3.0469e+00,\n",
      "        -2.5314e-01,  2.1150e+00,  3.3461e-01,  2.3056e+00,  1.0503e+00,\n",
      "        -5.9254e-01,  3.7503e+00, -5.5915e-01, -4.0962e-02, -9.5806e-01,\n",
      "        -6.4670e-01, -2.8114e-01,  9.1101e-01, -2.9091e-01,  6.2651e-01,\n",
      "         3.0983e+00, -2.7555e+00, -2.2084e+00, -1.4363e+00,  1.3696e+00,\n",
      "         1.1566e+00,  9.9163e-01, -8.8573e-01,  1.9868e+00,  1.7529e+00,\n",
      "        -8.9326e-01,  1.9845e+00, -4.2299e-01,  1.2361e+00,  1.1884e+00,\n",
      "         5.3038e-01, -4.0460e-01, -6.4267e-02,  2.3229e+00, -3.5375e-01,\n",
      "        -2.6384e+00, -1.9935e+00, -2.9284e+00, -2.6969e-01, -6.2545e-01,\n",
      "         9.6066e-01, -5.4623e-01, -2.0719e+00, -1.7087e+00, -1.4269e+00,\n",
      "         3.3480e+00, -1.0581e+00,  9.9269e-01,  2.7609e-01, -1.9966e+00,\n",
      "        -4.9012e+00, -9.6357e-01,  3.7882e-01, -1.7173e+00,  1.4351e+00,\n",
      "         1.2336e+00, -6.9744e-01,  3.1239e-01, -1.6550e+00, -1.8124e+00,\n",
      "         3.2391e+00, -1.1902e+00, -1.0349e+00,  1.3826e+00, -2.6365e+00,\n",
      "         8.0113e-01, -7.2803e-01,  5.9128e-01, -7.6371e-01,  6.4926e-02,\n",
      "        -1.2109e+00, -7.0997e-01,  1.7597e+00, -1.6433e+00,  4.2080e-02,\n",
      "        -3.2948e+00, -8.4896e-01,  2.8693e+00, -2.8339e-01,  2.4773e+00,\n",
      "         1.6248e-01,  8.4780e-01, -3.6511e-01,  1.7884e-01,  3.8772e-01,\n",
      "        -7.4396e-01,  1.5694e+00,  1.8718e-01,  1.6399e+00,  1.3538e+00,\n",
      "         3.8419e+00,  1.7026e+00,  9.0305e-01, -2.2296e+00, -5.3682e-01,\n",
      "         1.2284e-01,  2.0387e+00, -3.7733e-01, -2.8964e-01, -2.0172e-01,\n",
      "        -2.2648e-01,  8.7458e-01,  1.0855e+00,  2.9186e+00,  1.3012e+00,\n",
      "        -1.0498e+00,  1.8799e-01,  7.3090e-01,  2.0013e+00, -1.0896e+00,\n",
      "         7.3122e-01, -1.4497e-01,  9.2841e-02, -6.5270e-01, -9.8812e-02,\n",
      "        -2.0058e-01, -4.1719e-01,  8.2008e-01,  8.1446e-01, -1.1332e+00,\n",
      "         9.5902e-01, -2.2488e+00, -1.5737e+00, -1.8485e+00,  3.1490e+00,\n",
      "        -1.2816e+00, -2.0193e+00, -4.0844e-01,  1.3724e+00, -5.3925e-01,\n",
      "        -5.9841e-01, -6.3306e-01,  8.4911e-01,  5.8089e-01,  9.5757e-01,\n",
      "         1.3915e+00,  7.1798e-01,  8.9881e-01, -1.4916e-01, -1.0508e+00,\n",
      "         4.5521e-01, -1.1351e+00,  2.5905e-01,  3.5509e-01, -5.7738e-01,\n",
      "         2.8357e+00, -1.1012e+00, -2.7853e+00,  1.9913e+00, -8.7544e-01,\n",
      "        -2.2480e+00, -1.2594e+00,  1.8751e+00,  2.3171e+00, -2.3333e+00,\n",
      "         8.3120e-01,  1.0475e+00, -6.3099e-01, -9.1760e-01,  3.2224e-01,\n",
      "         5.9908e-01, -1.9434e+00,  2.1661e+00,  4.9589e-01,  4.2779e-01,\n",
      "        -5.5299e-01,  7.4578e-01,  6.8406e-01, -2.1005e+00, -2.3899e-01,\n",
      "         2.4937e-01, -1.4756e+00,  1.0050e+00, -6.8667e-02,  3.7625e+00,\n",
      "         1.7590e+00,  1.8866e+00,  2.6133e+00,  6.4647e-01,  3.2307e+00,\n",
      "        -4.4347e-01,  9.1494e-02, -5.0362e+00, -5.5574e-01,  2.3164e+00,\n",
      "         8.4688e-01, -3.6290e-01,  2.4119e+00, -1.7969e-01, -8.4557e-01,\n",
      "         1.0463e-01,  3.8735e-01,  6.7009e-01,  8.6999e-01,  1.4193e+00,\n",
      "         1.0982e+00,  9.2663e-01,  7.3452e-01, -1.7132e+00, -2.4354e+00,\n",
      "         5.7571e-01, -1.6885e+00, -2.9338e-01, -1.2275e-01, -1.8966e+00,\n",
      "        -1.3062e+00, -3.4151e-01,  5.5866e-01, -7.0523e-01, -2.6286e+00,\n",
      "         7.2168e-02,  5.6501e-02, -8.3409e-01, -2.1619e-01,  1.2930e+00,\n",
      "        -1.2786e+00, -4.2702e-01,  1.2588e+00, -3.2838e-01, -9.3127e-01,\n",
      "         1.1467e+00,  1.1903e+00, -1.2858e+00,  1.3787e+00, -1.7669e+00,\n",
      "        -6.0917e-01, -2.1381e+00, -6.5180e-01,  1.4387e+00, -1.9687e+00,\n",
      "         1.2185e+00, -1.1018e+00, -2.9871e-01,  1.0113e+00, -2.2422e+00,\n",
      "         8.7597e-01,  1.6177e+00, -7.8951e-01, -5.4638e-01,  3.1330e-02,\n",
      "        -7.9261e-01,  1.5886e+00,  2.0647e-01, -1.8771e+00, -9.4895e-01,\n",
      "        -4.2136e-01,  2.0768e+00, -2.3031e+00,  5.4089e-01,  1.8172e-01,\n",
      "        -1.1211e+00, -2.3013e-01, -4.7705e-01,  1.5144e+00,  7.7873e-01,\n",
      "         3.8644e-01,  3.3935e+00,  5.6187e-01, -8.4459e-01,  1.4734e+00,\n",
      "         6.0433e-01,  2.6588e-01,  5.3628e-01, -2.8552e+00,  1.4194e+00,\n",
      "        -1.2545e+00,  1.6000e-01, -8.8979e+00, -4.5803e-01, -8.2480e-01,\n",
      "         1.9581e-01,  6.8189e-02,  8.2083e-02, -1.5153e+00, -7.6444e-01,\n",
      "        -2.0347e+00, -1.8498e+00, -2.2650e-01,  8.7971e-01,  7.7981e-01,\n",
      "        -1.1818e+00,  1.5377e-01,  3.7818e+00]), tensor([-2.8452e+00, -1.2640e+00, -3.8979e-02, -8.8134e-01,  2.1694e+00,\n",
      "         6.6308e-01,  3.0543e-02,  3.6411e+00, -1.7894e+00,  7.0397e-01,\n",
      "        -6.7769e-02, -7.2143e-01,  4.9108e-01, -6.3811e-01,  2.6170e+00,\n",
      "         2.3653e+00, -1.6324e-01, -3.9934e-01, -4.4578e-01, -1.8825e+00,\n",
      "         3.8846e+00,  1.8913e+00, -3.5981e+00, -4.9905e-01,  5.6483e+00,\n",
      "        -1.6853e+00, -3.1464e-01,  4.9116e-01, -2.0721e+00, -1.6954e+00,\n",
      "        -1.2383e+00,  1.9606e+00,  1.5478e+00,  1.6540e-01, -7.8714e-01,\n",
      "        -1.8308e+00,  5.9611e-01, -3.1977e+00, -1.9669e+00,  3.7033e+00,\n",
      "        -1.1030e+00,  9.3979e-01,  2.7755e+00, -7.3200e-01, -6.9018e-01,\n",
      "        -9.7556e-01,  2.2543e+00, -1.8324e+00, -1.3947e+00, -1.1754e+00,\n",
      "        -6.2342e+00,  1.2297e+00, -7.3208e-01,  5.3780e-01, -5.4208e+00,\n",
      "         6.9118e-01,  1.0301e+00, -3.5507e+00, -8.4109e-01, -1.1938e+00,\n",
      "         1.5400e+00, -8.7650e-01,  9.5776e-02,  8.1633e-01,  1.8806e+00,\n",
      "         9.2960e-02,  1.0027e+00,  1.7719e+00, -2.5901e+00, -1.3227e+00,\n",
      "        -2.2222e+00, -3.7709e+00, -2.4971e-01,  1.3780e+00,  1.7040e+00,\n",
      "         1.2848e+00, -8.1794e-01, -2.9264e-01, -9.6827e-01, -3.0880e+00,\n",
      "        -9.9322e-01,  4.9189e+00,  9.9674e-01, -1.7551e-01,  2.7152e-01,\n",
      "         1.7632e+00, -2.0716e+00, -7.1546e-01, -2.6261e+00,  2.8266e+00,\n",
      "        -6.2739e-02, -1.1693e+00, -2.4484e+00, -1.3644e+00,  1.9939e+00,\n",
      "        -3.4022e+00, -1.7627e+00, -1.4058e+00,  1.4407e+00, -2.7837e+00,\n",
      "        -1.6271e+00, -6.8267e+00,  1.2149e+00,  4.8606e-01, -6.0399e-01,\n",
      "        -7.4446e-01,  2.7049e-01,  2.9481e+00,  2.7138e+00, -2.0161e+00,\n",
      "         7.7502e-01,  1.4114e+00, -1.1309e+00, -2.0801e-01, -1.6564e+00,\n",
      "         1.0361e+00, -2.2542e-01,  1.5206e+00,  1.2574e+00, -2.4135e+00,\n",
      "        -4.3554e+00,  8.5480e-02,  2.1764e+00,  1.8444e+00,  7.9934e-01,\n",
      "         9.5169e-01,  8.2401e-01,  1.7090e+00,  1.0391e+00, -2.3231e+00,\n",
      "         1.1034e+00,  4.8090e+00,  4.6027e+00,  8.8047e-01, -1.2062e+00,\n",
      "        -1.2406e+00, -1.2785e-01, -3.0164e+00, -2.5147e+00, -5.3232e-01,\n",
      "         2.3326e+00,  2.1155e+00,  3.8504e+00, -9.1041e-01,  6.4471e-01,\n",
      "        -4.0538e-01, -3.5999e+00, -3.2155e-01,  5.4544e-01, -1.0004e+00,\n",
      "         1.2032e-01,  7.4224e-01, -2.3189e+00, -9.6154e-01, -3.5557e-01,\n",
      "        -3.5921e-01, -3.7294e-01,  3.3181e+00,  6.4291e-01,  1.7078e+00,\n",
      "         2.5178e+00,  2.1138e+00,  2.1938e+00,  1.5699e+00,  1.4011e+00,\n",
      "        -2.7131e+00, -1.5166e+00, -1.3037e+00,  5.7364e-01, -3.3322e+00,\n",
      "         1.0064e+00, -2.9931e+00,  3.8201e+00,  2.4690e-01, -2.3059e+00,\n",
      "        -1.0037e+00,  2.0240e+00, -2.7316e+00,  1.2401e+00, -4.6453e-01,\n",
      "        -2.0267e+00,  7.2209e-01, -1.1397e+00, -1.8474e+00,  1.5697e+00,\n",
      "         1.9889e-01, -9.0094e-01, -4.4203e+00,  1.2660e+00,  4.6887e+00,\n",
      "        -1.2517e+00,  1.8309e+00,  1.8213e+00, -5.0100e+00,  1.7329e+00,\n",
      "        -3.6257e+00, -1.7435e+00, -2.7080e+00,  2.1207e+00, -1.1196e+00,\n",
      "         4.7052e-01,  3.9004e+00, -6.2597e-01, -6.9996e-01,  1.0586e+00,\n",
      "         4.4716e+00, -4.2026e-01,  4.8752e-02,  6.0263e-01, -9.5991e-01,\n",
      "        -5.9311e-01,  6.5446e+00, -8.4823e-01, -7.7244e-01,  2.7850e+00,\n",
      "         1.5578e+00, -2.2224e+00,  9.8064e-01,  6.2429e-01,  4.1197e-01,\n",
      "         5.7884e-01, -2.7948e+00, -5.1942e-02,  1.8936e+00, -2.0226e+00,\n",
      "         5.2329e+00,  1.3634e+00,  2.1116e+00,  2.0336e+00,  8.7080e-01,\n",
      "         5.4676e-01, -1.2530e+00, -2.9096e-01, -1.3417e+00, -8.6880e-01,\n",
      "         4.4428e+00,  5.1403e-01,  6.1882e-01, -1.8540e-01,  8.5748e-01,\n",
      "        -9.7306e-01,  1.2463e+00, -3.9883e-01,  7.2208e-01, -3.2755e+00,\n",
      "        -1.8679e+00, -1.4193e+00, -2.6487e+00, -3.0717e+00, -1.5947e+00,\n",
      "        -1.5368e+00,  7.0137e-01, -6.0947e-01, -2.2628e+00, -4.6446e+00,\n",
      "         5.8630e-01,  7.5746e-02,  2.4905e+00,  2.0670e+00, -5.3687e-01,\n",
      "        -9.8232e-02,  2.8103e+00,  1.1235e+00,  9.0859e-01, -2.6223e+00,\n",
      "         1.8782e+00, -1.1977e+00,  1.7466e+00,  3.0359e+00,  1.5818e+00,\n",
      "        -4.2850e+00, -3.3739e+00, -2.5660e+00, -1.2684e-01, -4.4317e+00,\n",
      "        -1.8966e+00,  7.5499e-01, -2.2511e+00, -2.2493e+00, -8.5348e-01,\n",
      "         5.4062e-01,  2.6084e+00,  1.3324e-01, -1.7490e-01,  1.3757e+00,\n",
      "        -1.4448e+00,  1.5491e+00, -1.5060e+00,  1.7903e-02,  1.1007e+00,\n",
      "        -2.4343e+00,  2.1384e+00, -3.4168e+00, -6.5310e+00,  3.9213e+00,\n",
      "         4.3551e-01,  1.3200e+00, -1.2707e-02, -2.3863e+00, -3.3629e+00,\n",
      "        -7.5305e-01, -2.9484e+00,  1.3388e+00,  6.4153e-02, -2.5455e+00,\n",
      "         3.7534e+00, -5.7889e-01, -1.8705e+00, -2.6876e+01,  6.5301e-01,\n",
      "        -1.7506e+00, -5.6501e-01, -2.1815e+00, -1.5123e-02, -1.5995e-01,\n",
      "        -9.1659e-01,  2.6590e-01, -2.1839e+00, -2.8600e+00, -2.7853e+00,\n",
      "        -1.0027e+00,  1.8100e+00,  1.1351e+00, -1.3416e+00,  1.2292e+00,\n",
      "         3.0742e+00,  1.0119e+00,  3.6661e+00,  2.7597e-02, -6.4669e-01,\n",
      "         5.2245e-01, -1.6260e+00, -4.5012e+00,  5.4700e-01, -1.0526e+00,\n",
      "         2.1778e+00, -1.7333e+00, -1.2888e+00,  8.0751e-01, -4.4512e-02,\n",
      "         6.8494e-01,  2.1217e+00, -2.4898e+00,  2.0946e+00, -1.2178e-02,\n",
      "        -1.4004e+00,  3.3850e+00,  1.7011e-01, -8.7827e-01,  6.0233e-01,\n",
      "         2.3959e+00,  6.8397e-01,  1.6943e+00,  8.8507e-01, -3.8392e-01,\n",
      "        -1.0406e+00,  2.1103e-02,  1.8650e+00, -2.0512e+00, -2.4932e+00,\n",
      "         2.1059e+00,  5.0636e-02,  1.7535e+00,  2.5390e+00,  2.4509e-01,\n",
      "         4.7676e-01,  1.7260e+00, -6.7423e-01,  2.1984e+00, -2.0084e+00,\n",
      "        -3.8422e+00,  1.0669e+00,  1.0303e+00, -4.8957e+00, -9.7772e-01,\n",
      "        -1.1264e+00, -1.1055e+00,  1.0973e+00, -3.3858e+00,  3.3139e-01,\n",
      "         3.4369e+00, -1.1490e+01, -2.0305e+00, -2.0933e+00,  2.0658e+00,\n",
      "         4.1198e-01,  3.7367e+00, -2.7347e+00, -1.4851e+00, -2.7786e+00,\n",
      "         2.0399e+00, -5.3514e-01,  1.5881e+00,  4.7507e-01,  3.3890e-01,\n",
      "        -1.6550e+00, -2.0421e+00, -1.1198e+00, -3.9222e+00,  2.6379e+00,\n",
      "        -1.7221e+00,  1.7150e+00, -8.2098e-01,  3.9538e-02,  1.5913e+00,\n",
      "        -3.5293e+00, -4.6255e-02,  1.2693e+00,  1.1228e+00,  2.7464e-01,\n",
      "         3.3322e+00,  1.2006e+00,  4.0727e+00,  1.9034e-01, -3.2234e+00,\n",
      "         3.5301e+00, -1.0699e-01,  3.0865e+00, -2.4141e+00, -1.6515e+00,\n",
      "        -2.3613e+00,  1.7934e-01, -3.2007e+00,  2.7246e+00, -1.7255e+00,\n",
      "         1.0240e+00,  2.4183e+00, -9.9813e-01, -2.6642e-01,  2.8130e+00,\n",
      "        -3.0611e+00, -2.9618e+00, -3.0118e+00,  1.8048e-01, -1.9621e+00,\n",
      "         3.0944e+00, -2.8613e+00,  1.4506e+00, -1.7162e+00, -2.1216e-01,\n",
      "        -2.2296e+00,  1.1874e+00,  1.6829e+00, -1.0667e+00, -3.6975e-01,\n",
      "         4.4359e+00,  4.7046e+00, -2.7451e+00,  6.4402e-01,  2.4903e+00,\n",
      "        -8.4704e-02,  2.1845e+00,  2.8837e+00,  3.3572e+00, -1.0583e+00,\n",
      "        -3.8393e+00, -1.0814e-01,  6.4332e-03, -4.2181e-01, -1.9786e+00,\n",
      "         1.2534e+00, -2.9735e+00,  1.8855e+00, -2.2994e-01,  2.4846e+00,\n",
      "        -5.3481e-01, -7.5822e-02, -2.2384e+00,  2.2386e+00, -5.8480e-01,\n",
      "        -1.0062e+00, -2.2558e+00,  2.2065e+00, -1.5541e-01, -2.6430e-01,\n",
      "         7.7272e-01, -1.0574e+00, -1.3100e+00, -1.8110e+00,  1.7437e+00,\n",
      "         2.0413e+00, -2.2723e+00,  1.0305e+00, -1.7276e+00,  1.5431e+00,\n",
      "        -1.1100e+00,  7.1602e-01,  3.3040e+00,  1.7099e+00,  1.5933e+00,\n",
      "         2.1972e+00,  1.3980e+00,  1.2773e+00,  2.7742e-01, -1.2866e+00,\n",
      "         4.5078e-01, -1.3305e-01,  1.7587e+00, -1.0916e+00,  1.4130e+00,\n",
      "        -9.9946e-01, -2.7838e+00, -3.3949e+00, -2.8962e+00, -4.8977e-01,\n",
      "        -3.5344e-01,  2.0875e+00,  1.0096e+00,  8.1989e-01,  3.2773e+00,\n",
      "        -3.0082e+00, -2.8939e+00, -2.6407e-01,  2.0185e+00, -2.0218e-02,\n",
      "         2.3646e-01,  1.3385e+00,  8.4954e-02, -1.6192e+00,  3.6624e-01,\n",
      "        -2.3894e+00, -4.6229e+00, -1.8267e+00, -2.2384e+00, -1.4836e+00,\n",
      "         3.4846e+00, -2.5451e+00, -2.8119e+00, -1.8797e+00, -4.4010e-02,\n",
      "         9.4922e-01,  4.2904e-01,  4.8778e-01, -3.0535e+00,  2.1034e+00,\n",
      "        -4.0661e+00,  2.0767e+00,  3.1153e-01, -8.0113e-01, -3.2627e+00,\n",
      "         8.9030e-01,  6.3890e-01, -1.1628e+00, -6.3916e+00, -2.1050e+00,\n",
      "        -7.1399e-01,  1.1567e-01, -1.2455e+00,  2.3352e+00, -3.2480e+00,\n",
      "         2.9960e+00, -8.2380e-01, -3.7505e+00, -4.1110e+00, -3.2720e-01,\n",
      "        -2.2214e+00,  1.7440e+00,  2.2666e+00,  9.1215e-02,  3.6715e+00,\n",
      "        -2.5617e+00,  2.5893e-02,  1.0741e+00, -2.2690e+00,  1.9019e+00,\n",
      "        -3.5889e-01, -3.6624e-01,  2.2263e+00,  2.9867e+00,  1.4235e+00,\n",
      "         3.2180e+00,  5.5358e+00, -1.3980e+00,  1.7902e+00, -1.3524e-01,\n",
      "        -8.3683e-02,  1.1163e+00,  1.6610e+00, -3.5499e+00,  2.3010e+00,\n",
      "        -2.7790e-01,  3.3679e+00,  3.6647e+00, -1.5322e+00, -1.0675e+00,\n",
      "        -1.2678e+00, -1.8771e+00,  2.2714e-01, -6.3034e-01,  1.1181e+00,\n",
      "        -4.7906e+00,  5.5936e-01, -5.2320e-01,  1.7388e+00, -3.5581e+00,\n",
      "        -5.1506e-01,  2.0459e+00, -1.4232e+00, -1.5975e+00,  4.6497e-01,\n",
      "         5.4066e-01,  9.8165e-01,  5.7275e-02, -6.2168e-01,  3.1260e+00,\n",
      "         2.7656e-01,  1.7214e+00, -8.9373e-01, -4.2589e-01,  1.3227e+00,\n",
      "        -2.5645e+00, -2.7516e-01, -7.4395e-01,  6.0892e-01,  3.6775e+00,\n",
      "         7.8041e-01, -2.5113e-01,  1.6679e+00,  4.5699e-02,  1.7842e+00,\n",
      "         2.0935e+00,  5.5058e-01,  1.5998e+00, -4.9311e-01,  2.4400e+00,\n",
      "        -1.1945e+00, -1.3504e+00,  2.1328e+00, -2.3329e+00,  4.8528e-01,\n",
      "         5.6852e-01, -1.4764e+00, -4.4476e+00, -1.7010e+00, -2.2254e+00,\n",
      "        -5.6627e-01, -1.9902e+00, -6.2271e-01,  4.9124e-01, -4.2691e-02,\n",
      "        -2.2023e+00,  3.5352e+00, -5.1454e-01,  2.0652e+00, -5.8519e-01,\n",
      "        -1.9224e-01,  1.9758e+00,  3.5564e-01, -1.2007e+00,  8.8423e-01,\n",
      "         1.6653e+00,  2.0417e+00, -1.5993e+00, -1.9710e+00, -4.9650e-03,\n",
      "        -1.9539e-01, -3.3054e-02,  1.3537e+00,  2.9335e+00,  2.5254e+00,\n",
      "         2.3171e+00, -1.0524e+00,  9.1621e-01,  3.7208e+00,  2.2047e+00,\n",
      "         1.6794e+00,  3.4282e+00, -4.1557e-01,  1.3909e+00,  4.2882e-01,\n",
      "         6.1806e-01, -3.3787e-01, -2.2373e-01, -1.0536e+00,  3.6082e+00,\n",
      "         2.9607e+00,  1.9464e+00, -1.8017e+00, -9.6962e-01,  2.3714e+00,\n",
      "         4.6671e-01,  3.0984e-02, -2.6206e+00,  3.1884e+00, -4.0927e-01,\n",
      "         6.1104e-01, -1.1036e+00,  1.4120e-01,  8.7290e-01, -3.9184e-01,\n",
      "         1.2200e+00, -5.2623e-01, -1.9524e-01, -2.7441e+00, -3.3019e-01,\n",
      "        -6.3876e-01,  9.3043e-01,  1.8821e+00, -8.6678e-01, -3.3256e-01,\n",
      "        -3.3349e-01, -1.6410e+00,  5.2065e+00,  5.9826e-01, -2.0110e+00,\n",
      "         1.0594e+00,  2.3949e+00, -2.5817e+00,  2.5070e+00, -1.2681e-01,\n",
      "         9.9169e-01, -3.3370e+00, -1.1062e+00, -1.1389e-01, -2.1951e+00,\n",
      "         2.3174e+00, -3.2340e-01, -2.5456e+00,  1.9663e+00,  2.2411e+00,\n",
      "         2.3044e-01, -2.5150e+00, -3.7194e+00, -2.0407e-02, -3.9071e-02,\n",
      "        -2.7566e+00, -1.9173e+00, -1.2632e-02, -2.2258e+00, -3.1597e+00,\n",
      "         6.5778e-01,  4.2710e+00,  8.8912e-01, -1.3246e+00,  6.2786e-01,\n",
      "        -8.8503e-01, -3.0033e+00, -2.8306e+00,  1.5186e+00,  1.9260e+00,\n",
      "         8.8534e-01,  5.6951e-01,  2.0639e-01, -4.6501e+00,  2.0706e+00,\n",
      "        -1.3354e+00,  8.7714e-01, -1.8146e+00, -2.6292e+00,  5.3605e+00,\n",
      "        -1.1161e+00,  1.1599e+00, -1.1158e+00, -6.2836e-01,  4.1761e-01,\n",
      "        -2.0316e+00,  2.6071e+00, -3.6564e-01,  1.6600e+00,  2.0254e-01,\n",
      "        -1.5705e+00, -3.5744e+00, -2.7562e+00,  6.1471e+00,  1.7218e+00,\n",
      "        -1.9352e+00,  1.5490e+00,  2.9721e+00]), tensor([-2.2699e+00, -1.4770e+00,  1.6387e+00,  6.0575e-02, -1.5868e+00,\n",
      "        -1.0916e+00, -1.4772e+00,  2.6634e+00, -1.6105e+00, -1.4968e+00,\n",
      "         4.3701e-01, -7.2393e-01, -2.0814e+00,  9.9456e-01, -1.3925e+00,\n",
      "         3.3535e-01, -1.5351e+00, -1.9652e+00,  4.5411e-01, -1.7569e+00,\n",
      "         2.4109e+00,  2.7369e+00, -3.7554e+00, -1.3133e+00,  8.0185e-01,\n",
      "         6.5100e-01,  4.3778e-01, -6.7957e-01, -1.2046e+00,  4.1092e-01,\n",
      "         1.0374e+00,  1.4964e+00,  1.7174e+00, -8.9124e-01, -1.1090e+00,\n",
      "        -2.6990e+00,  1.1303e+00, -1.8807e+00, -3.3551e+00,  1.4522e+00,\n",
      "        -1.0616e+00, -1.4261e+00,  2.0464e+00,  5.4351e-01, -6.1709e-01,\n",
      "         2.8202e-01,  3.8584e+00, -1.6919e+00,  2.5313e-01, -2.9162e-01,\n",
      "        -6.3372e+00, -1.0835e+00,  1.2499e+00,  2.1532e+00, -1.4147e+00,\n",
      "         1.4551e+00,  8.4753e-01, -3.0983e+00, -3.2261e-01, -1.9717e+00,\n",
      "         4.6460e-01,  8.4711e-02,  3.6461e-01,  1.7869e+00,  4.4247e+00,\n",
      "         1.4965e+00, -6.0574e-01,  1.8463e-01, -1.9898e+00,  1.4305e+00,\n",
      "         2.1036e-01, -2.0606e+00,  2.2796e+00,  5.5288e-01, -3.8456e-01,\n",
      "         1.9470e+00, -1.0019e+00, -1.2767e+00,  1.8876e+00, -2.0473e+00,\n",
      "        -1.5775e+00,  5.1150e+00,  8.0710e-01, -3.3880e-01,  1.4421e+00,\n",
      "        -3.0983e+00, -2.1203e+00, -1.7010e+00, -2.0835e+00,  3.7680e-01,\n",
      "         1.5861e+00, -1.4165e+00,  1.3790e-01, -2.3110e+00,  1.5981e+00,\n",
      "        -4.1739e+00, -1.4359e+00, -1.1476e+00,  1.2907e+00, -1.9964e+00,\n",
      "        -1.6438e+00, -4.8100e+00,  2.5931e+00, -4.5793e-01, -1.7812e+00,\n",
      "        -1.2567e+00, -1.9695e+00,  4.9483e+00,  2.0924e+00, -2.0803e+00,\n",
      "         1.5019e+00,  1.3322e+00, -2.5280e-01, -5.2411e-01, -1.5625e+00,\n",
      "         7.7742e-01, -4.6360e-02, -4.7467e-01,  1.1335e+00, -2.5055e+00,\n",
      "        -1.1906e+00,  1.9768e-01, -3.6302e-01,  2.0443e+00,  6.5094e-01,\n",
      "         2.0780e+00, -1.7054e+00,  2.2884e+00, -1.3354e-02, -2.3608e+00,\n",
      "         2.3630e+00,  1.2959e+00,  5.0885e+00, -1.3057e+00,  2.3367e+00,\n",
      "         5.5011e-01, -5.1495e-01, -9.6656e-01, -5.3831e+00,  7.9633e-01,\n",
      "         1.5519e+00,  1.0273e+00,  3.1113e+00, -1.9113e+00, -1.6377e+00,\n",
      "        -1.6836e+00, -4.3391e+00,  2.9858e-01,  2.7705e+00, -3.8104e-01,\n",
      "        -8.8126e-01,  1.6506e+00, -1.0560e+00, -9.7600e-01,  1.2454e+00,\n",
      "        -1.5926e+00,  8.0105e-01,  1.9582e+00, -6.4106e-01, -3.2233e-03,\n",
      "         1.1354e+00,  1.3997e+00, -1.8471e-01,  9.8288e-01,  1.0344e+00,\n",
      "        -1.5904e+00,  2.8947e-02,  1.4988e+00,  1.4916e+00, -2.7144e+00,\n",
      "         1.5174e-01, -6.6386e-01,  1.4011e-01, -9.1182e-01,  1.2659e+00,\n",
      "         1.3155e+00, -2.2421e+00, -8.9243e-02,  3.2075e+00,  6.2787e-01,\n",
      "        -8.5265e-01,  2.0534e-01, -8.6316e-01, -4.0543e+00, -4.4177e-01,\n",
      "         9.5307e-01, -1.2959e+00, -1.1560e+00, -6.0646e-01,  2.3515e+00,\n",
      "        -1.1326e+00,  2.7539e+00,  5.9726e-01, -2.6856e+00,  1.8328e-01,\n",
      "        -9.6384e-01, -1.2116e+00, -3.7741e+00, -1.2253e+00, -2.4618e+00,\n",
      "         3.0295e+00,  3.2536e+00,  2.3077e+00, -1.0290e+00, -1.4100e+00,\n",
      "         2.9148e+00,  1.1864e+00, -3.2240e-01, -1.2780e+00,  5.4761e-01,\n",
      "         1.1196e+00,  5.0114e+00, -7.6878e-01, -1.8553e-01,  2.9880e+00,\n",
      "         2.2588e+00, -3.4290e-01, -2.1431e+00,  6.7916e-01,  1.7656e+00,\n",
      "         9.0090e-01, -4.0461e+00, -2.4176e+00,  6.9020e-01, -2.0262e+00,\n",
      "         3.1174e+00,  1.0706e-02,  2.7278e+00,  2.9600e+00, -8.5031e-01,\n",
      "         4.4101e-01, -1.8773e-01,  2.8457e-01,  1.9084e+00,  6.6968e-01,\n",
      "         3.9433e+00, -6.6054e-01,  1.7246e+00,  2.5190e+00,  1.0995e+00,\n",
      "         2.2770e+00,  2.1340e+00,  4.1854e+00, -1.3106e+00, -6.1835e-01,\n",
      "        -1.7958e+00, -1.8217e+00, -1.4559e+00, -4.1903e+00, -1.4768e+00,\n",
      "        -1.2638e+00, -2.4173e+00, -3.2673e-01, -7.6974e-01, -3.9928e+00,\n",
      "        -3.3891e-02, -2.4831e+00,  1.7601e-02,  1.4836e+00,  1.1913e-01,\n",
      "        -3.3467e-01,  1.9113e+00, -1.2961e+00, -2.8420e-01, -2.3659e+00,\n",
      "         4.2077e-01,  1.9226e+00,  1.9038e+00,  4.8571e+00,  1.2023e+00,\n",
      "        -2.0527e+00, -3.0869e+00, -1.3213e-01,  1.6417e+00, -1.4472e+00,\n",
      "         1.1887e-02, -1.8010e+00, -2.3983e+00, -2.8123e+00,  1.8049e+00,\n",
      "         2.5488e+00, -8.6723e-01, -7.6083e-01, -1.3884e+00,  1.0049e+00,\n",
      "        -8.0486e-01,  2.2258e-02, -1.1688e+00, -2.5592e+00, -1.9120e+00,\n",
      "        -4.2781e+00,  2.7913e+00, -1.7628e+00, -7.6966e+00,  5.0150e+00,\n",
      "        -1.1245e+00, -2.1722e-02,  1.0091e+00, -2.0825e+00, -1.5955e+00,\n",
      "         7.4781e-01, -4.3286e+00,  4.5438e-01, -1.7178e+00, -1.0253e+00,\n",
      "         4.7185e-01, -1.8509e+00,  5.4408e-01, -3.1868e+01, -2.5706e+00,\n",
      "        -2.4792e-01, -7.7802e-01,  2.9752e-01,  2.4103e-01,  5.7334e-01,\n",
      "        -3.9468e-02, -1.4462e+00, -3.1467e-01, -2.3562e+00, -2.5010e+00,\n",
      "        -1.7994e+00,  1.7388e+00,  2.2662e+00, -3.7923e+00, -2.1530e+00,\n",
      "         1.7171e+00,  4.6672e-02,  5.5565e+00, -2.9764e+00, -1.3486e+00,\n",
      "        -2.0250e-01, -2.5156e+00, -3.1792e+00,  1.8185e+00,  7.7841e-01,\n",
      "         1.8032e+00, -1.5159e+00, -5.2146e+00, -1.0245e+00,  1.2703e+00,\n",
      "         1.6412e+00,  2.5882e+00, -1.5524e+00,  7.2163e-02, -7.0855e-01,\n",
      "        -1.3999e+00,  2.8615e+00, -9.5886e-01, -1.1882e+00,  6.1916e-01,\n",
      "         2.1285e+00, -1.3779e-01,  2.1775e+00,  4.9182e-02,  5.0174e-01,\n",
      "        -2.8610e+00, -2.5018e-01,  4.3605e+00, -3.2033e+00, -2.3031e+00,\n",
      "         3.4912e-01, -1.4417e+00,  1.8631e+00,  1.4833e+00,  2.2943e+00,\n",
      "         8.1011e-01,  2.0441e+00, -1.9284e+00,  1.5360e+00, -2.7310e+00,\n",
      "        -2.3954e+00,  3.8972e+00,  5.7832e-01, -1.9791e+00, -3.9883e+00,\n",
      "        -1.7300e+00,  6.4184e-01,  2.3045e-01, -1.0293e+00,  1.4307e+00,\n",
      "         1.4267e+00, -1.1744e+01, -2.3918e+00, -1.6989e+00,  3.0980e+00,\n",
      "        -2.8763e-01, -7.4686e-01, -5.8421e-01, -2.8751e+00, -5.2407e+00,\n",
      "         6.1495e-02,  2.0536e+00,  1.1094e-01, -5.5202e-02, -5.3762e-01,\n",
      "         3.9269e-01,  9.7102e-01,  7.0111e-01, -9.8874e-01,  3.2770e+00,\n",
      "        -2.2740e+00,  2.1684e+00, -2.3912e+00,  9.0267e-01,  3.3481e+00,\n",
      "        -2.0721e+00,  1.3691e+00,  2.5730e+00,  4.0841e+00,  1.1093e+00,\n",
      "         1.4415e+00,  4.5657e+00,  5.2375e+00,  3.0786e+00, -4.3004e+00,\n",
      "         2.0630e+00,  2.1891e+00,  3.1104e+00,  8.2556e-01, -1.5785e+00,\n",
      "        -1.1773e+00, -4.0249e-01, -3.8753e+00,  3.3128e+00, -1.9459e+00,\n",
      "         1.3508e+00,  2.3362e+00, -2.1339e+00, -1.0139e+00,  3.9274e-02,\n",
      "        -2.0279e+00, -3.3901e+00, -5.6899e+00, -6.9015e-01, -1.8529e+00,\n",
      "         8.6332e-01, -1.5294e+00,  1.8800e+00, -8.6355e-01,  3.3888e+00,\n",
      "         1.4087e+00,  2.0582e+00,  3.4011e-01, -4.1195e-01, -1.0397e+00,\n",
      "         5.6841e+00,  7.0523e-01, -1.4605e+00,  5.0360e-01,  2.1277e+00,\n",
      "        -1.6099e+00, -1.0950e+00, -2.4652e+00,  5.5059e+00,  1.8665e+00,\n",
      "        -1.9161e+00,  2.0273e+00, -1.6416e+00, -2.8588e+00,  2.8606e-01,\n",
      "         2.7208e+00, -5.6214e-01,  2.7588e-02, -6.8149e-01, -2.6383e-01,\n",
      "        -1.1112e+00, -2.3596e+00, -4.8115e+00,  3.5079e+00,  4.7762e-01,\n",
      "         8.8655e-02,  4.8775e-02,  1.5350e+00,  7.4463e-01, -6.2408e-01,\n",
      "         1.0805e+00, -1.5812e+00, -1.4396e+00, -1.7029e+00,  9.4137e-01,\n",
      "         5.4100e-01, -1.4394e+00,  3.1976e+00,  4.3753e-01,  4.9502e-01,\n",
      "        -1.3417e+00,  8.2697e-01,  1.9167e+00,  1.5060e+00, -5.3164e-01,\n",
      "        -5.7762e-01, -1.0706e-02,  4.6130e-01,  7.2863e-02,  8.6573e-03,\n",
      "         1.4765e+00, -3.7741e-02, -1.8587e-01, -3.4776e+00,  7.0618e-01,\n",
      "        -6.5119e-01, -4.3646e-02, -8.8199e-01, -3.0458e+00,  5.2171e-01,\n",
      "         1.3162e+00, -1.2961e+00,  4.3597e-01,  1.1854e+00,  8.8496e-01,\n",
      "        -1.1542e+00, -1.8635e+00,  1.2075e-01,  1.9475e+00, -8.0249e-01,\n",
      "        -1.3664e+00, -5.2767e-02, -1.9227e+00,  1.0369e+00, -3.3157e-01,\n",
      "         8.8248e-01, -2.4385e+00, -1.8177e-01,  1.8846e-01,  2.2483e+00,\n",
      "         1.4492e+00, -3.5757e+00,  4.7759e-01, -1.3905e+00, -3.9607e-01,\n",
      "         2.8230e+00, -1.0356e+00,  1.0189e+00, -2.1151e+00,  3.2764e-01,\n",
      "        -4.4705e+00,  1.6418e+00, -8.1445e-01, -2.7836e-01, -2.1115e+00,\n",
      "        -4.4943e-01,  3.1860e-01, -3.1737e+00, -3.2494e+00, -1.7354e+00,\n",
      "         4.9493e-01,  9.6474e-01, -9.6847e-01, -1.1352e+00, -2.3345e+00,\n",
      "        -3.1103e-01, -1.0077e+00,  4.9171e-01, -5.1830e+00,  1.2147e-02,\n",
      "        -1.9636e+00, -5.4592e-01,  2.0160e+00,  1.1779e+00,  2.7127e+00,\n",
      "        -8.0284e-02, -1.5851e+00,  5.1267e-01, -2.6708e+00, -2.2790e+00,\n",
      "         2.9587e-01,  3.5116e-01,  7.0063e-02,  2.3640e+00,  4.3061e-01,\n",
      "        -1.8362e-01,  2.2023e+00, -5.2937e-02,  1.8717e+00,  3.3464e+00,\n",
      "         2.4211e+00,  1.8221e+00,  1.7369e+00, -2.3785e+00,  1.6621e+00,\n",
      "         1.2279e+00,  2.8412e+00,  2.6681e+00, -1.9571e+00, -3.7941e-01,\n",
      "         4.9577e-01, -5.6257e-02, -1.7084e+00, -2.6815e+00, -9.9537e-01,\n",
      "        -5.0302e+00,  2.8447e+00,  1.6234e+00,  1.3289e+00,  4.7150e-01,\n",
      "         1.0160e+00,  7.5838e-01, -1.8139e+00, -3.0636e+00,  1.2209e-01,\n",
      "        -2.7261e+00, -4.9445e+00,  4.1588e-02, -5.8211e-01,  3.4734e+00,\n",
      "         9.7760e-01,  7.2451e-01, -3.1182e+00,  6.9760e-01,  3.6487e+00,\n",
      "         1.7993e+00, -7.2334e-01, -3.8553e+00,  1.2418e+00,  9.6081e-01,\n",
      "         7.4534e-01, -2.6882e-01,  1.1919e+00,  6.1676e-01,  1.4535e-02,\n",
      "         1.5667e+00,  9.7842e-01,  1.5603e+00, -1.7706e+00,  9.3040e-01,\n",
      "        -1.7286e+00, -8.5646e-01,  1.9600e+00, -2.4760e+00, -1.2918e+00,\n",
      "         2.4196e+00, -2.9381e-01, -3.2045e+00,  5.1243e-01, -7.7940e-01,\n",
      "        -3.4658e+00, -1.9396e+00,  2.0232e-01,  6.1334e-01, -2.6809e-01,\n",
      "        -2.1300e+00,  1.5688e+00,  2.7904e-01, -1.2830e+00,  1.2979e+00,\n",
      "        -2.0427e+00,  5.3692e+00, -8.8514e-01, -7.9402e-01, -1.4901e+00,\n",
      "         1.3385e+00,  3.3449e+00, -7.0691e-01, -2.9176e+00,  3.2777e-01,\n",
      "        -1.2353e+00, -5.0140e-01,  3.2521e+00,  1.2679e+00,  1.0573e+00,\n",
      "         2.2273e+00,  5.7793e-01,  2.7098e+00, -1.3135e+00,  1.2276e+00,\n",
      "         3.3378e+00,  1.6906e+00,  8.2113e-01,  2.8012e+00,  1.9140e+00,\n",
      "         1.9759e+00, -1.3317e+00, -3.7470e-02,  1.2006e+00,  3.7145e+00,\n",
      "         4.0414e+00,  1.2070e+00, -1.7283e+00, -8.5071e-01, -3.0900e-01,\n",
      "         1.4780e+00,  5.4563e-01,  1.4805e+00,  3.4561e+00, -5.7594e-01,\n",
      "        -5.3491e-01, -1.9884e+00, -5.6164e-01,  2.0172e+00, -1.8297e+00,\n",
      "         8.3575e-01, -2.5385e+00,  8.0577e-01, -2.8787e+00, -8.9256e-01,\n",
      "        -3.3818e+00,  7.7648e-02,  6.6218e-02, -1.4479e+00,  7.0568e-02,\n",
      "        -1.2109e+00, -3.2228e+00,  3.9164e+00,  1.0208e-01, -9.2500e-01,\n",
      "         1.2347e+00,  3.9907e+00, -1.1733e+00,  2.2651e-01, -2.6181e+00,\n",
      "        -7.8625e-01,  5.3682e-01, -5.4309e-02,  2.1793e-01, -8.5257e-01,\n",
      "         2.4189e+00, -9.1652e-01, -2.5272e+00,  3.4936e+00,  1.8370e+00,\n",
      "        -2.5127e-01, -2.0485e+00, -1.2892e+00,  1.7599e+00, -4.4496e-01,\n",
      "        -3.1213e+00, -2.8077e+00, -4.2407e-01, -3.9146e+00, -4.2480e+00,\n",
      "         7.5254e-01,  3.1728e+00,  6.3810e-01, -1.5381e-01,  8.1765e-01,\n",
      "        -5.3260e-01,  1.1568e+00, -5.5539e-01,  2.8090e+00,  2.1420e+00,\n",
      "        -8.4961e-01,  5.3157e+00, -3.5367e-01, -4.3880e+00,  1.0052e+00,\n",
      "         2.0279e+00,  3.8561e-01,  1.3978e-01, -1.5604e+00,  5.5198e+00,\n",
      "        -2.9852e-01, -7.0407e-01, -1.0247e+00, -4.4341e-02, -7.4506e-02,\n",
      "         2.1832e+00,  1.0369e+00,  1.1918e+00,  8.4564e-02,  1.2934e-01,\n",
      "         8.5856e-01, -2.3753e+00, -1.4778e+00,  2.3754e+00,  2.3779e+00,\n",
      "        -1.2590e+00,  5.1942e-01,  5.3610e+00]), tensor([-1.2828e+00, -1.9436e+00,  3.2306e+00,  4.7966e-01,  1.8438e-01,\n",
      "        -2.5832e+00,  6.1421e-01,  1.5379e+00, -2.1891e+00,  1.6652e+00,\n",
      "         2.6193e-01, -3.8228e+00, -1.5508e+00,  6.1659e-01, -6.1673e-01,\n",
      "        -9.5672e-01, -1.3903e+00, -3.0585e+00,  2.8542e+00, -3.4964e+00,\n",
      "        -8.9934e-02,  5.9683e-01, -5.0234e+00,  2.5188e-01,  1.8035e+00,\n",
      "         2.7437e+00,  5.5397e-01,  5.7513e-01,  4.2121e-01,  3.4578e-02,\n",
      "         1.2004e+00,  1.7879e+00, -2.9506e-01, -2.1180e+00,  1.9010e+00,\n",
      "        -2.7341e+00,  1.0412e+00, -2.1745e+00, -1.2688e+00,  1.5005e+00,\n",
      "         1.0867e+00, -2.4087e+00,  3.7773e+00, -1.6926e+00, -3.0376e+00,\n",
      "        -3.5069e-01,  3.4545e-01, -3.4461e+00, -1.5010e+00, -1.6375e+00,\n",
      "        -4.1769e+00, -2.1139e+00, -9.0922e-02,  9.0809e-01, -2.6722e+00,\n",
      "         1.3437e-01,  6.2858e-01, -4.0091e+00,  6.2096e-01, -1.3685e+00,\n",
      "         1.0368e+00,  1.0859e+00,  2.6921e-01,  1.3349e+00,  4.1514e+00,\n",
      "         6.2985e-01,  5.8185e-01,  1.2607e-02, -2.9181e+00,  1.3704e+00,\n",
      "        -1.4028e+00, -3.5965e+00,  6.4413e-01,  7.1749e-01, -1.8099e+00,\n",
      "        -3.9349e-01,  5.4515e-01, -2.2818e+00, -1.9040e-02, -1.0983e+00,\n",
      "        -8.3771e-01,  1.1523e+00, -1.7034e+00,  1.7030e+00, -1.2515e+00,\n",
      "         1.0631e+00, -2.8287e+00,  3.9834e-01, -3.6244e+00, -1.6522e+00,\n",
      "         9.3888e-01,  5.3446e-02, -9.7368e-01, -2.0556e+00,  1.3977e+00,\n",
      "        -1.7861e+00,  2.5593e-01, -3.1240e+00,  1.2708e+00, -2.5495e+00,\n",
      "         3.6022e-01, -3.1187e+00,  2.5922e+00,  2.8346e+00, -9.3943e-01,\n",
      "        -3.3212e+00, -1.3524e-01,  2.2085e+00,  3.5995e-01, -5.9241e-01,\n",
      "         1.3840e+00,  1.8941e+00,  8.0408e-01, -1.7805e+00, -1.7567e+00,\n",
      "         2.7090e+00, -2.4264e+00,  5.7166e-01,  1.9391e+00, -1.1685e+00,\n",
      "         6.5202e-01,  4.5145e-01, -2.0061e-01, -2.3564e-01,  2.1121e+00,\n",
      "         3.8351e+00, -2.6468e+00,  3.7361e+00, -7.3645e-01, -2.4241e+00,\n",
      "         5.6707e-01,  1.1842e+00,  4.5334e+00, -2.4290e-01,  2.4534e+00,\n",
      "        -2.4998e-01,  3.6316e-01, -2.8274e+00, -5.2715e+00,  1.0678e+00,\n",
      "        -2.3670e+00, -3.1100e+00,  2.7890e+00, -2.7576e+00,  1.7594e+00,\n",
      "        -2.0587e+00, -1.0409e+00, -9.5077e-01,  2.7995e+00,  1.6203e+00,\n",
      "        -9.4403e-01,  2.8147e+00, -2.8719e+00, -1.8273e+00,  9.8021e-01,\n",
      "        -1.0631e+00, -7.3229e-01,  2.6472e+00, -1.3943e+00,  4.7888e-01,\n",
      "         2.5704e+00,  3.6745e+00,  3.5455e-01,  1.5735e-01,  2.1174e+00,\n",
      "        -6.5078e-01,  1.6539e+00,  3.0568e+00, -4.8404e-01,  1.3665e+00,\n",
      "        -1.0466e+00, -4.7662e-01,  2.6885e+00,  1.3856e+00, -1.4517e+00,\n",
      "         8.4021e-01,  1.6153e-01,  3.8364e-02,  1.7242e+00, -3.6040e-01,\n",
      "        -1.3024e-01,  1.6396e+00,  6.9388e-01, -2.8054e+00, -1.1241e+00,\n",
      "        -1.0594e+00, -1.1212e+00, -5.7208e-01,  2.1523e+00,  3.4206e+00,\n",
      "        -1.0946e+00, -1.1062e+00, -1.2822e+00, -1.7484e+00, -5.8902e-01,\n",
      "        -4.3546e-01, -9.8626e-01, -1.6470e+00,  2.9314e-01, -8.9536e-01,\n",
      "         2.7807e+00,  1.5845e+00,  3.4113e+00, -2.1966e+00, -2.2836e+00,\n",
      "         1.5026e+00,  2.1041e+00,  3.0551e-02, -1.8021e+00,  2.0407e+00,\n",
      "        -1.5504e+00,  4.6062e+00,  1.4612e+00,  6.0768e-02,  2.2018e+00,\n",
      "         3.2676e+00, -2.5204e+00, -5.6526e-01,  2.0188e+00,  6.2420e-02,\n",
      "         2.8556e+00, -1.3617e+00, -1.4599e+00, -1.0611e-01, -7.0692e-01,\n",
      "         4.4835e+00,  1.2284e+00,  7.5523e-01,  7.4561e+00,  4.1846e-01,\n",
      "         9.0219e-01, -1.3646e+00, -7.5555e-01,  3.7313e+00,  8.6089e-01,\n",
      "         2.5982e+00, -1.8695e+00, -6.0165e-02,  2.3619e+00,  9.5799e-02,\n",
      "         2.3442e+00,  1.1275e+00,  1.4438e+00,  7.0205e-01,  1.7908e-01,\n",
      "        -1.8512e+00, -3.3609e+00,  3.7149e-01, -1.5386e+00, -2.6635e+00,\n",
      "        -1.7547e+00, -3.7641e+00,  8.9424e-01, -1.8195e+00, -2.9674e+00,\n",
      "         2.0448e-01, -3.0494e+00, -2.6735e+00,  1.3385e+00,  2.6870e+00,\n",
      "        -1.7893e-01,  2.6310e-01, -1.7428e+00, -2.0141e+00, -1.8292e+00,\n",
      "         1.3211e+00,  2.2683e+00,  1.2094e+00,  3.7943e+00,  2.4847e-01,\n",
      "        -7.9136e-01, -1.8998e+00, -2.4637e+00,  3.6095e+00, -4.5473e+00,\n",
      "         1.0047e+00, -1.3653e+00, -2.7387e+00, -8.5308e-01,  3.4947e+00,\n",
      "         3.1279e+00, -8.7535e-01, -2.2832e+00,  5.8852e-01,  1.8714e+00,\n",
      "        -2.0784e+00,  6.7361e-01,  2.1963e+00, -2.2111e+00, -2.4430e+00,\n",
      "        -2.0869e+00,  1.0778e-01, -1.5326e+00, -5.6689e+00,  2.9326e+00,\n",
      "        -6.1273e-01,  6.5137e-01,  4.8005e-01, -1.0933e+00, -2.4018e+00,\n",
      "         4.7648e-01, -2.5582e+00, -2.8403e+00,  1.5659e+00,  8.1841e-01,\n",
      "         8.8866e-01, -3.1319e+00, -3.0572e+00, -3.4071e+01, -1.1104e+00,\n",
      "         4.1601e-01, -2.5190e+00,  5.3625e-01,  1.0515e+00, -3.1136e+00,\n",
      "        -9.5158e-01, -1.3414e+00, -7.8402e-01, -5.2174e+00, -1.5909e+00,\n",
      "         6.5507e-02,  5.1612e-01,  2.2144e+00, -1.4627e+00, -2.5596e+00,\n",
      "         2.2673e-01,  1.9817e+00,  1.6924e+00, -5.9040e-01,  1.9029e+00,\n",
      "        -4.9784e-01, -4.3939e+00, -2.9282e+00,  3.0271e+00, -3.9081e+00,\n",
      "         4.2505e-01, -1.1872e+00, -2.5410e+00, -2.8987e-01,  3.2746e-01,\n",
      "         2.8340e+00, -2.7464e-01, -1.8551e+00,  1.8967e+00,  2.8747e-02,\n",
      "        -1.9426e+00,  2.4286e+00, -7.4656e-01, -4.9015e-01,  1.2944e+00,\n",
      "         2.9018e+00,  2.1291e+00,  1.0215e+00, -5.8750e-01, -3.8184e-02,\n",
      "         4.9198e-01, -8.7644e-01,  3.5868e+00, -2.0874e+00, -1.9529e+00,\n",
      "         8.4567e-01, -6.0807e-01,  4.2758e-01,  7.8730e-01,  3.9378e+00,\n",
      "         2.0505e+00,  1.7476e+00, -3.3620e-01, -1.3934e+00, -4.3405e+00,\n",
      "        -8.9029e-01,  4.1427e+00,  1.3494e+00, -3.8981e+00,  7.4506e-02,\n",
      "        -5.0312e+00,  4.5659e-01,  3.3999e-02, -1.3687e+00,  2.2628e+00,\n",
      "         1.0487e+00, -1.1277e+01, -5.3630e-01, -3.8069e+00,  6.1207e-01,\n",
      "        -1.1169e-02,  1.8967e+00, -1.7799e+00, -2.3858e+00, -2.6879e+00,\n",
      "        -7.9706e-02, -5.3245e-01,  2.0744e+00,  1.3810e+00, -3.8403e-02,\n",
      "         1.4848e+00,  6.1185e-01, -9.2788e-01, -2.1325e+00,  4.5358e+00,\n",
      "        -8.3184e-01,  1.1866e+00, -6.6570e-01, -2.9297e-01,  4.7784e+00,\n",
      "        -3.6356e+00,  2.0383e+00,  2.4547e+00,  9.3533e-01, -1.2571e+00,\n",
      "         1.6319e+00,  3.4577e+00,  1.3573e+00,  4.1227e-01, -6.0946e+00,\n",
      "         2.4162e+00,  2.6162e+00,  4.5966e+00,  2.5832e+00, -4.8408e-01,\n",
      "        -1.0972e+00, -1.1346e+00, -2.6390e+00,  2.3535e+00, -7.7046e-01,\n",
      "         5.6208e-01,  3.8556e+00,  1.0778e+00,  2.5027e-01,  1.4830e+00,\n",
      "        -1.1885e+00, -4.3333e+00, -3.9680e+00, -3.8896e-01, -2.1563e+00,\n",
      "         1.0604e+00, -1.6062e+00,  7.7671e-02, -1.9279e+00,  2.4476e+00,\n",
      "         2.2738e+00,  4.2705e-02,  1.0437e+00, -2.3190e+00, -3.0257e-01,\n",
      "         5.2393e+00,  3.0223e+00, -2.1716e+00, -3.0963e-01, -1.2011e+00,\n",
      "         1.3666e+00, -2.4934e+00, -1.6853e+00,  1.2991e+00,  1.2003e+00,\n",
      "        -2.7065e+00,  9.6610e-01, -2.3792e+00, -2.6342e+00, -4.2479e-03,\n",
      "         3.0313e+00, -9.8553e-01,  1.2200e-01, -1.8126e+00,  3.5632e+00,\n",
      "        -1.8735e+00, -1.2264e+00, -1.7430e+00,  1.3942e+00,  4.7571e-01,\n",
      "         3.1504e+00, -1.0138e+00,  1.6249e+00, -5.4756e-01, -1.7036e+00,\n",
      "         2.0104e+00, -2.3250e+00, -1.7984e+00, -3.7033e+00, -2.3579e-01,\n",
      "         1.8161e+00, -2.2072e+00,  2.1863e+00,  2.4870e+00,  1.8690e+00,\n",
      "        -6.1405e-01, -1.4474e+00, -4.7329e-01,  1.5845e+00, -1.2857e+00,\n",
      "         2.9665e+00,  2.5020e-01, -1.4629e-01,  4.2591e-01, -2.0116e+00,\n",
      "         4.4017e+00, -3.5114e-01, -4.0921e-01, -3.8978e-02,  2.5301e+00,\n",
      "        -2.1332e+00, -1.1916e+00, -8.1310e-01, -5.9637e-01,  1.3185e+00,\n",
      "         6.1976e-01, -1.3278e+00, -1.6388e-01, -6.0028e-01,  1.0154e+00,\n",
      "        -1.1318e+00, -2.7269e+00,  1.0917e+00, -1.8428e-01, -2.8819e+00,\n",
      "        -8.0103e-01, -1.2079e+00, -1.7595e+00,  1.3970e+00, -1.2049e+00,\n",
      "        -5.0726e-01, -4.2256e+00,  1.6333e-01, -4.0379e-01,  2.3942e+00,\n",
      "         1.2086e+00, -3.4081e+00,  6.9015e-01,  2.1589e+00, -2.0192e+00,\n",
      "        -1.1585e+00, -1.3736e+00, -4.7692e-01,  1.2749e-02, -2.0290e+00,\n",
      "        -3.8194e+00,  2.8264e+00,  1.0896e+00,  1.4826e+00, -3.0791e+00,\n",
      "        -9.3816e-01,  5.9005e-01, -1.1206e+00, -3.5315e+00, -3.1293e+00,\n",
      "         8.9517e-01, -7.1511e-01, -6.8928e-01, -2.3230e+00, -3.4919e-01,\n",
      "        -3.6872e+00, -6.8826e-02,  2.4197e+00, -6.5394e+00,  1.2862e+00,\n",
      "        -2.6993e+00, -4.9346e-01,  2.2592e+00, -1.0375e+00,  1.8044e+00,\n",
      "        -2.6992e-01, -2.2474e+00,  1.0890e+00, -5.6893e-02, -2.3629e+00,\n",
      "         2.0604e+00, -6.1118e-01,  2.3601e+00,  1.7179e+00, -7.9553e-01,\n",
      "         1.5976e+00,  1.9227e+00, -1.0088e+00,  1.2776e+00,  3.7854e+00,\n",
      "         2.8998e+00,  6.0081e-02,  3.8729e+00, -1.6958e+00,  3.8182e+00,\n",
      "        -3.2218e-01,  3.1517e+00,  2.6220e+00, -2.0082e+00,  6.4619e-01,\n",
      "        -1.5509e+00, -2.1493e+00,  1.9049e+00, -2.2588e+00, -3.4369e-01,\n",
      "        -2.3780e+00,  4.3883e-01,  3.1738e+00, -5.9038e-01,  5.3468e-02,\n",
      "        -1.4356e+00,  1.4951e+00, -2.3781e+00, -3.3532e+00,  1.7853e+00,\n",
      "        -1.8427e+00, -2.0096e+00, -4.0782e-01, -1.9089e-02, -5.0088e-01,\n",
      "         4.3938e-01,  8.8430e-01, -1.3932e+00, -7.1903e-01,  2.6205e+00,\n",
      "         1.3057e+00,  8.5216e-01, -2.5083e+00,  5.1793e-01, -1.2289e+00,\n",
      "        -1.6330e+00,  1.1993e+00,  2.0461e+00, -1.4362e+00, -1.1750e+00,\n",
      "         2.3223e+00,  1.7520e+00, -2.1701e-01, -4.3570e-01,  2.7512e+00,\n",
      "         7.8666e-01, -1.3579e+00,  3.6330e-01, -1.5447e+00, -1.3599e+00,\n",
      "         8.3062e-01, -9.0753e-01, -2.4015e+00,  1.2398e+00,  1.2701e-01,\n",
      "        -3.9236e+00, -2.3357e+00,  6.2621e-01,  8.2502e-01,  1.2489e+00,\n",
      "        -1.9722e+00, -1.5276e+00, -3.8568e-01, -2.6977e-01, -1.7093e+00,\n",
      "        -3.0812e+00,  5.2783e+00,  1.4201e+00,  3.1930e-01, -1.1909e+00,\n",
      "         2.8457e+00,  3.1038e+00, -1.0940e+00, -4.3245e+00,  1.5902e+00,\n",
      "        -5.4467e-01, -1.2071e+00,  4.4542e+00, -1.7595e-01,  1.0275e+00,\n",
      "         1.3922e+00, -1.1209e-01,  1.9933e+00,  5.2248e-01,  3.1041e-01,\n",
      "         3.0584e+00,  5.5880e+00,  1.5487e+00,  1.7837e-01,  2.2628e+00,\n",
      "         1.8349e+00, -2.1754e+00,  1.4723e+00,  9.0904e-01,  1.3929e+00,\n",
      "         1.1338e+00, -2.4392e-02, -4.9704e-01, -3.2054e-01, -5.2255e-01,\n",
      "         3.3508e+00,  4.3900e+00,  9.6956e-01,  1.1830e+00,  1.0747e+00,\n",
      "         2.7866e+00, -1.0224e-01,  1.2164e+00, -4.3556e-01, -1.0094e+00,\n",
      "        -3.0786e-01, -2.5533e+00, -5.5977e-01, -1.7025e+00, -2.1091e+00,\n",
      "        -1.3624e+00, -1.6043e+00, -1.5864e+00, -1.0641e+00, -2.0692e-01,\n",
      "        -1.2187e+00, -3.8351e+00,  4.9557e+00,  7.3223e-01,  1.2125e+00,\n",
      "         1.8182e+00,  4.4507e+00, -3.0510e+00,  8.2702e-01, -6.2825e-01,\n",
      "         3.2736e-02, -1.4972e-01, -1.6467e+00,  1.3640e-01,  3.9067e-01,\n",
      "         3.4357e+00, -2.4572e+00, -2.1942e+00,  4.8935e+00,  1.8160e+00,\n",
      "         3.5799e-01,  6.1754e-02, -1.9356e+00,  1.6082e+00,  8.7550e-01,\n",
      "        -2.6350e+00, -1.5745e+00,  1.6022e+00, -4.2524e+00, -2.8650e+00,\n",
      "         2.0366e+00,  2.4890e+00,  7.2563e-01, -2.6791e+00,  8.6033e-01,\n",
      "        -1.0488e+00, -1.1577e+00,  7.3811e-01,  1.2196e+00,  1.4138e+00,\n",
      "        -5.9329e-01,  3.0971e+00, -2.4302e-01, -3.4987e+00,  1.1565e-01,\n",
      "        -9.4162e-01, -8.0061e-01,  2.3564e-01, -8.2261e-01,  2.8886e+00,\n",
      "         9.3521e-01, -4.2830e-01, -2.3302e+00,  2.5372e-01,  1.7506e+00,\n",
      "         1.6853e+00, -2.8402e-01, -7.0734e-01, -9.6309e-01,  1.3410e+00,\n",
      "         1.4572e+00, -1.6467e+00, -2.5080e+00,  3.5759e+00,  7.0337e-01,\n",
      "         5.2350e-01,  1.3808e+00,  7.4770e+00]), tensor([ 3.4332e+00, -3.6702e+00,  5.1524e-01, -1.9301e+00,  9.1972e-01,\n",
      "         2.1628e+00, -2.8406e-01, -3.5489e-01, -1.2888e-01,  5.1582e-01,\n",
      "        -1.2025e+00,  8.3292e-01, -1.3111e+00,  8.2539e-01,  5.0989e-01,\n",
      "         1.6283e+00, -4.1111e-01,  8.9060e-01,  5.2052e+00, -1.6043e+00,\n",
      "         7.3996e-01,  1.1995e-01, -2.9153e+00,  2.6288e+00, -9.4253e-01,\n",
      "        -2.1711e+00, -2.9729e-01, -1.3270e-01, -5.3163e-01,  1.3076e+00,\n",
      "        -3.7453e-01,  2.4881e+00, -4.1910e-01,  1.0831e+00, -5.2189e+00,\n",
      "        -6.2937e-01,  1.3761e+00, -1.7644e-01, -2.5708e+00, -2.1733e+00,\n",
      "        -1.1613e+00, -2.8333e+00,  1.6189e+00,  1.0322e+00, -1.7110e+00,\n",
      "        -1.0313e-02, -2.3094e+00, -2.1098e+00,  7.0696e-01, -3.2037e+00,\n",
      "        -4.1202e+00,  3.8694e+00, -2.1443e+00, -6.5272e-02, -5.7439e-01,\n",
      "         3.1723e+00, -1.6145e+00, -2.7471e+00,  2.2075e+00, -2.1520e+00,\n",
      "        -9.9475e-01,  1.9335e+00,  7.0473e-01, -1.5063e+00,  1.6961e+00,\n",
      "         1.6324e+00,  2.5599e+00,  1.8188e+00, -2.6413e+00, -3.4501e+00,\n",
      "        -2.0792e+00,  1.3420e+00, -1.0879e+00,  1.5594e+00, -2.7478e+00,\n",
      "         1.2852e+00, -1.8133e-01, -2.2710e+00, -2.1618e-01, -2.0413e+00,\n",
      "         1.6047e+00,  3.5359e-01, -3.0541e-01,  1.1297e+00,  1.6201e+00,\n",
      "         4.6058e-01,  2.6668e+00, -2.0639e+00, -9.1710e-01, -1.9180e+00,\n",
      "        -7.0799e-01, -1.0539e+00, -1.1449e+00, -3.4228e-01,  2.7490e+00,\n",
      "         2.3775e+00, -1.8882e+00,  9.3462e-01, -4.7797e-02,  3.4133e-01,\n",
      "        -9.0085e-01, -4.6913e+00,  1.2547e+00,  8.0022e-01, -8.0002e-01,\n",
      "        -7.8572e-01, -7.3049e-01,  1.3487e+00, -7.1783e-01, -2.7920e-01,\n",
      "         9.5746e-01,  2.8473e+00,  1.0781e+00, -8.4001e-01, -1.9892e+00,\n",
      "        -4.9281e-01, -1.5808e+00,  5.1277e-01,  3.7697e+00,  3.7915e-01,\n",
      "        -1.1063e+00,  1.6024e+00,  2.7587e-01,  1.7941e+00,  1.2476e+00,\n",
      "         2.0422e+00,  7.3845e-01,  2.7240e-01,  6.5024e-01,  2.5191e-01,\n",
      "        -1.5426e+00,  1.9062e+00,  2.2215e+00,  4.6143e-01, -2.8548e+00,\n",
      "        -2.9783e+00,  1.4346e+00, -1.4570e+00, -6.8127e+00,  6.7747e-01,\n",
      "        -3.0490e+00,  1.3705e+00,  2.7019e+00, -3.5649e-01,  1.6112e+00,\n",
      "        -7.2029e-01, -5.3529e+00,  6.9660e-01, -1.7300e+00,  1.4991e+00,\n",
      "         1.4193e-01,  3.8759e+00, -4.3290e-01,  2.6316e-01, -2.6396e+00,\n",
      "         5.8008e-01, -3.6108e+00,  2.0855e+00, -6.3658e+00,  1.6203e+00,\n",
      "         5.2045e-01,  1.0006e-01,  1.3970e+00,  3.9485e+00,  2.5436e+00,\n",
      "         3.4670e+00,  2.0099e+00,  1.4980e+00,  6.2022e-02,  4.9667e-01,\n",
      "        -9.9392e-01,  6.9043e-01, -1.2046e+00,  1.3333e-01, -3.6136e+00,\n",
      "        -3.6496e-01, -1.9353e-02, -5.8857e-01,  1.2758e+00, -1.3413e+00,\n",
      "        -2.0494e+00,  1.2821e+00, -1.0845e+00,  6.2492e+00, -2.2623e+00,\n",
      "         4.6271e-01,  2.9607e+00, -2.0029e+00, -2.9572e-01, -1.4681e+00,\n",
      "        -3.2686e-01, -1.2536e+00, -1.4237e+00, -4.7360e+00, -1.8182e+00,\n",
      "         1.2310e+00, -3.1627e+00, -5.1830e-01, -1.1555e-01, -2.9616e-01,\n",
      "         2.4420e+00,  1.5962e+00,  2.3796e+00,  2.9427e+00,  2.8749e-01,\n",
      "         2.1585e+00,  3.0005e+00, -5.0689e-01, -1.4654e+00,  3.3378e+00,\n",
      "        -1.5533e+00,  3.1677e+00,  1.8270e+00, -7.3547e-01, -7.2044e-01,\n",
      "         1.0941e+00,  5.1861e-01, -1.6328e+00,  3.2680e+00, -3.3388e+00,\n",
      "         2.3131e+00, -3.3136e+00, -2.1597e+00, -1.1092e+00, -2.0956e+00,\n",
      "         2.4628e+00,  2.9712e+00, -1.4501e+00,  3.1171e+00,  9.4707e-01,\n",
      "        -1.6758e+00, -2.9671e+00,  9.4666e-01, -4.3528e-02,  4.8776e+00,\n",
      "         3.1593e+00, -6.8705e-01,  1.9171e+00,  1.4689e+00, -1.6778e+00,\n",
      "        -9.8265e-01,  1.1232e+00, -1.1081e+00, -2.5597e-01,  3.4376e-01,\n",
      "        -7.2935e-01,  8.2364e-01,  1.1172e+00, -1.6751e+00, -2.1706e+00,\n",
      "        -7.0229e-01, -4.7654e+00,  9.4947e-01,  3.1813e+00, -4.3683e+00,\n",
      "         9.4147e-01, -1.8986e+00, -1.4564e+00,  4.2075e-01,  3.3722e+00,\n",
      "        -2.0520e+00,  2.6629e+00,  2.7347e+00,  9.6407e-01, -4.2355e-01,\n",
      "        -2.1707e+00,  3.8129e+00,  5.4410e+00,  1.5721e+00,  1.8808e+00,\n",
      "         1.1069e+00, -5.6518e+00,  1.6085e+00,  3.0449e+00, -3.9098e+00,\n",
      "         1.4464e+00,  1.5132e+00, -1.9804e+00, -2.7858e+00,  2.4288e+00,\n",
      "         1.6855e+00,  1.7261e+00, -1.0102e+00,  1.0293e+00, -2.6357e-02,\n",
      "        -4.3154e+00,  2.1575e+00, -1.5476e+00,  4.2280e-01, -1.0065e+00,\n",
      "        -1.3271e+00,  3.0828e+00, -3.8382e+00, -3.5256e+00,  3.6836e+00,\n",
      "        -8.2911e-01,  1.3833e+00,  3.2643e+00,  1.4403e+00, -2.8472e+00,\n",
      "         4.5444e-01, -4.2079e-01,  3.0689e+00,  2.9553e+00, -9.5775e-01,\n",
      "         1.4831e+00, -1.2334e+00, -9.5638e-01, -2.6171e+01, -1.0409e+00,\n",
      "         1.0498e-01,  3.8715e-02,  5.4021e-01,  5.9818e-01, -2.5506e+00,\n",
      "         3.1777e-01, -2.4084e+00,  2.0282e+00, -1.7111e+00, -5.6521e-01,\n",
      "        -1.7878e+00, -1.5787e+00,  7.4520e-01, -6.6914e-01, -4.8891e+00,\n",
      "        -3.5792e-01,  2.3204e+00,  3.8741e+00,  1.8225e+00, -1.8667e+00,\n",
      "        -1.8966e+00, -2.3043e+00, -1.6037e+00,  1.8624e-01, -3.5487e+00,\n",
      "         1.0164e+00, -2.5970e+00,  4.1001e-01, -8.6087e-01, -1.7853e+00,\n",
      "        -1.9051e-01,  7.6074e-01, -2.3068e-01,  3.3373e-01, -5.4808e-01,\n",
      "        -7.5957e-01,  5.8677e-01,  2.0515e+00, -1.9580e+00, -3.1403e+00,\n",
      "        -2.2898e+00,  3.8407e+00,  3.9033e-01, -1.6828e+00, -9.8132e-05,\n",
      "         1.9476e+00, -3.3645e-01,  1.6651e+00, -2.5267e-01,  1.5364e+00,\n",
      "         3.1005e+00, -2.3464e-01,  2.9476e-01,  4.2453e-01,  1.3563e+00,\n",
      "         2.9370e-01,  6.5230e-01, -2.0051e+00, -1.7952e+00, -2.4215e+00,\n",
      "        -2.1560e+00, -3.7076e-01,  2.3746e+00, -1.6280e+00, -6.1793e+00,\n",
      "        -9.6055e-01, -7.0504e-01, -3.1744e-01,  3.4686e-01, -3.2648e+00,\n",
      "         1.3114e+00, -1.2242e+01, -2.9494e+00, -9.3927e-01,  1.9187e+00,\n",
      "         7.4928e-01,  6.4951e-01, -1.1238e-01, -5.4955e-04, -3.7263e+00,\n",
      "         5.4573e-01, -3.7122e-02,  1.3131e+00, -1.6474e+00, -2.0681e+00,\n",
      "        -9.7385e-01, -2.0093e+00,  2.1386e+00, -8.7048e-02,  1.8298e+00,\n",
      "         2.4956e+00,  1.8302e-02, -4.4555e-01,  1.0643e+00,  2.5553e-02,\n",
      "        -5.9829e-01, -1.6033e+00, -7.0785e-03,  2.8300e+00, -2.1838e+00,\n",
      "        -1.3715e-01,  4.2531e-01, -3.2053e+00, -1.2669e+00, -5.7428e+00,\n",
      "         2.9310e+00, -8.6625e-01,  3.0505e+00,  2.4316e+00, -5.7429e-01,\n",
      "        -1.8887e+00, -2.0986e-01,  6.5814e-01,  4.7287e-01, -1.2379e+00,\n",
      "        -5.2455e-01,  2.7603e+00,  8.9089e-01, -1.7778e+00,  2.9189e-01,\n",
      "         8.9800e-02, -3.2666e+00, -2.4275e+00, -1.3960e+00, -6.6660e-01,\n",
      "         1.9152e-01,  8.9849e-02, -1.2779e+00, -2.3345e+00,  2.1146e+00,\n",
      "         2.3768e-01,  8.0226e-01,  4.5967e-01,  1.1037e+00, -1.6887e+00,\n",
      "         5.3361e+00,  1.2629e+00, -1.6032e+00,  2.8381e+00,  2.9687e-02,\n",
      "         5.3145e+00, -3.3242e+00,  5.2182e-01, -3.2100e-01,  9.5009e-01,\n",
      "        -1.1225e+00,  1.5694e+00,  1.3799e+00,  2.3488e+00,  8.1106e-01,\n",
      "         2.7983e+00, -3.0202e-01, -2.6499e-01,  2.3528e+00,  6.1422e+00,\n",
      "        -2.7135e+00,  2.1943e-01, -2.7635e+00,  5.8156e-01,  1.7960e+00,\n",
      "        -1.4246e-01, -2.4873e+00, -6.6798e-02,  1.1983e+00, -2.1999e+00,\n",
      "         2.1006e-01,  2.3463e+00, -2.5426e+00, -2.8292e+00, -1.9084e+00,\n",
      "         3.4959e+00,  2.6582e+00,  1.1312e+00, -9.2082e-01,  3.2273e+00,\n",
      "         5.6458e-01,  1.3418e+00,  2.4434e+00, -1.4211e+00,  1.2800e+00,\n",
      "        -2.4599e+00,  9.5561e-01,  4.9458e-01,  2.3132e+00,  1.1543e+00,\n",
      "        -4.9996e-01,  4.5152e-01, -1.9584e+00,  3.0361e-01,  1.3232e+00,\n",
      "        -1.2201e-01, -1.9967e-01,  6.3144e-01, -1.8458e+00,  5.4685e-01,\n",
      "         4.0933e+00,  1.1811e-01,  1.4363e+00, -1.3555e+00, -4.5125e+00,\n",
      "        -5.1725e-02,  4.9662e-01, -1.1244e+00, -1.1127e+00,  3.5235e-01,\n",
      "         1.7924e+00, -1.5628e-01,  9.3407e-01, -1.6585e-01, -2.6380e+00,\n",
      "        -1.5535e+00, -2.0369e+00,  2.4498e+00, -6.0272e-01,  4.8684e-01,\n",
      "        -1.5044e+00, -1.6277e+00,  2.5474e-01, -9.5895e-02, -2.1250e+00,\n",
      "        -8.1697e-01,  2.6131e-01, -1.1726e+00, -4.0544e+00,  3.0057e+00,\n",
      "        -1.5357e+00, -1.7445e+00,  1.2914e+00, -5.8117e-01, -1.7655e+00,\n",
      "        -1.8754e+00,  1.5628e-01, -2.6586e+00, -1.0960e+00, -5.4792e+00,\n",
      "        -9.2814e-01, -4.3030e-01, -9.8333e-01,  3.2377e-01,  1.4940e+00,\n",
      "         1.7593e+00,  1.2516e+00,  1.3756e+00, -1.4115e+00,  1.1134e+00,\n",
      "        -1.0067e+00,  6.9277e-01,  1.0135e+00,  1.4568e-01,  1.2696e+00,\n",
      "        -1.1376e+00, -4.5738e-01,  2.9685e+00, -8.1513e-01,  7.6922e-01,\n",
      "        -2.6390e+00, -1.2036e+00,  5.1756e+00, -1.3356e+00,  1.4856e+00,\n",
      "         1.6630e+00,  1.9101e+00, -7.2420e-01, -2.9201e-01, -1.9618e+00,\n",
      "         3.0985e+00, -1.3340e+00,  1.5846e+00, -3.7827e+00, -3.5177e-01,\n",
      "         1.0457e+00,  2.2443e+00, -3.1940e+00, -8.0133e-01, -3.8971e+00,\n",
      "        -1.1673e+00, -2.0848e+00,  4.5134e+00, -2.7376e-01, -2.2144e+00,\n",
      "         2.3394e-01, -1.2387e+00, -1.7301e+00,  2.2485e+00, -2.4181e+00,\n",
      "        -1.1076e+00, -2.7897e-01,  9.7153e-01, -1.9320e-01, -1.8969e+00,\n",
      "         1.1315e+00, -6.7340e-01, -2.1418e+00,  1.4048e+00,  8.9784e-01,\n",
      "         1.7980e+00,  3.5099e+00, -1.3184e+00, -2.1234e+00,  2.1170e+00,\n",
      "        -3.3913e-01,  1.0222e+00, -2.8400e+00,  8.7218e-01, -2.8660e-01,\n",
      "        -1.2828e+00, -2.1291e-01,  5.9595e-02,  2.4483e+00, -1.6348e+00,\n",
      "         1.8822e+00, -1.7739e+00, -1.4842e+00, -1.5095e+00,  9.8596e-01,\n",
      "        -1.2397e+00, -8.5784e-01,  3.2355e+00, -2.0285e+00,  9.3629e-01,\n",
      "         1.9471e+00, -3.8039e+00, -2.9288e+00,  3.6855e+00, -1.2923e+00,\n",
      "         3.1424e-01,  1.6151e+00, -3.6889e-01,  2.6424e+00,  1.1753e+00,\n",
      "         1.8279e-01,  1.2145e+00, -3.8372e-03,  2.0135e-01, -1.1370e+00,\n",
      "         1.7175e+00,  1.6871e+00, -1.0434e+00,  6.3020e-02, -7.2687e-01,\n",
      "        -1.7710e-01, -1.6421e-01, -2.0404e+00, -1.8336e+00,  1.1481e+00,\n",
      "         1.4689e+00, -3.4929e-01,  3.6705e+00, -4.6714e+00, -8.9353e-01,\n",
      "         2.5435e+00, -6.4699e-01, -2.0831e+00, -2.3471e+00, -1.0719e+00,\n",
      "         1.2520e+00,  5.2213e-01,  3.3925e+00,  1.8779e+00, -4.4870e-01,\n",
      "        -8.3231e-01, -1.1092e+00,  1.4772e+00,  1.6161e+00,  3.1655e+00,\n",
      "        -2.2493e+00,  4.4623e-01, -2.9404e-01, -1.8648e+00, -7.5480e-01,\n",
      "         1.0808e+00,  3.4122e+00,  2.4525e+00,  2.6607e+00,  2.5534e+00,\n",
      "         2.6118e-01, -7.0186e-01,  9.5171e-01,  2.7717e+00,  1.1385e+00,\n",
      "         5.8741e-01,  5.8194e-01, -8.7075e-01,  9.1600e-01, -4.6293e-01,\n",
      "         5.0844e-01,  3.0961e-01, -1.9436e+00, -1.9547e-02,  3.1868e-01,\n",
      "         7.7569e-01, -2.2714e+00,  2.3291e+00, -4.2492e-01, -1.0894e+00,\n",
      "         3.5575e-01,  2.1441e+00, -3.7954e+00,  8.9401e-01, -1.2782e+00,\n",
      "        -4.4319e-01, -5.7417e-01, -9.9094e-01,  2.4546e+00, -8.6315e-01,\n",
      "         5.5059e+00, -4.6601e+00, -4.0787e+00,  7.9423e-01,  2.4892e+00,\n",
      "        -4.7266e-01,  4.0881e+00, -1.7943e+00,  2.8430e+00,  2.3671e+00,\n",
      "        -3.6319e+00,  1.4226e+00, -1.7225e+00, -4.8481e+00, -4.4344e+00,\n",
      "        -3.6861e-01,  1.8029e+00,  4.5753e-01, -6.2085e+00, -8.7459e-02,\n",
      "        -2.0629e+00,  2.0627e+00, -3.9965e-01,  2.8208e+00, -7.2364e-01,\n",
      "        -8.5657e-01,  3.5650e+00, -1.1667e+00, -5.5953e+00,  1.2331e+00,\n",
      "        -1.7299e+00, -1.3627e+00, -2.1075e+00, -2.2476e+00,  3.1346e+00,\n",
      "         4.5992e+00,  3.8764e+00, -1.5315e-01,  7.3439e-02,  5.3196e-01,\n",
      "        -6.0912e-02,  3.2079e-01,  3.1804e+00,  2.2703e+00, -1.8331e+00,\n",
      "         3.4524e-01, -3.4687e+00, -1.7111e+00,  1.7845e-02,  2.9291e+00,\n",
      "        -1.0385e+00,  7.1017e-01, -3.6657e+00]), tensor([-9.7453e-01,  4.3186e-01, -2.7827e+00, -2.4595e+00, -1.0403e+00,\n",
      "         1.7042e+00,  2.8600e+00,  6.7459e-01, -7.5271e-01, -3.8627e-01,\n",
      "        -2.6770e-01,  1.8827e+00,  1.0606e+00, -1.0405e+00,  4.7790e-02,\n",
      "        -5.7330e-01, -1.0395e+00,  4.2372e-01,  6.8733e-01, -2.4630e-01,\n",
      "         2.0276e+00,  1.8767e+00, -1.2057e+00, -2.9438e+00,  2.9742e+00,\n",
      "         1.6593e+00, -5.2435e-01, -1.4278e+00, -2.2776e+00,  4.0313e-01,\n",
      "         2.7049e+00,  3.0912e+00,  2.1401e+00,  2.5955e+00, -4.6260e+00,\n",
      "        -2.1237e+00,  3.0963e-01, -5.5616e-01, -2.2900e+00,  2.5555e+00,\n",
      "         1.1763e+00, -3.6379e-01, -1.2749e+00, -2.6454e+00, -1.8239e+00,\n",
      "        -1.3160e+00,  3.3187e+00, -3.0024e+00,  3.0756e-01, -4.0786e-01,\n",
      "        -5.4375e+00, -2.7010e-01, -4.0195e+00,  3.1111e+00, -8.4212e-01,\n",
      "        -8.6663e-01,  1.2601e+00, -2.8336e+00, -9.1840e-01, -1.1144e+00,\n",
      "         1.4258e+00, -1.3402e-02, -1.9861e-01,  2.7283e-01,  1.5367e+00,\n",
      "         1.2592e+00,  2.8431e+00, -2.8774e-01, -5.6496e+00,  4.8782e+00,\n",
      "        -2.8825e+00,  6.7529e-01,  1.0666e+00, -2.5724e+00, -1.5761e+00,\n",
      "        -8.9784e-02, -7.3301e-01,  5.5236e-01,  1.3579e+00,  3.4460e-01,\n",
      "        -2.4451e+00,  3.9985e+00,  1.0765e+00, -2.8193e+00,  3.1944e-01,\n",
      "        -2.4678e+00,  6.6434e-01, -1.9162e-01, -3.6016e+00,  4.2242e+00,\n",
      "         4.7125e-01,  2.0034e+00, -2.2822e+00, -5.6263e-01,  3.3915e+00,\n",
      "         9.2402e-01, -1.9962e+00, -1.8788e+00,  7.9152e-01, -5.2180e-01,\n",
      "        -4.6661e+00, -6.3791e+00,  1.2983e+00,  8.5912e-01, -1.0345e+00,\n",
      "        -1.2016e+00, -1.2737e+00, -5.6586e-01,  6.4844e-02,  8.8432e-01,\n",
      "         1.7489e-01,  1.1361e+00,  2.2020e+00, -3.8859e+00, -4.8248e-01,\n",
      "         2.1312e+00, -1.8135e+00,  1.8181e+00,  8.7376e-01,  2.6201e-02,\n",
      "        -4.0663e+00, -2.0297e-01,  1.5659e+00,  2.9737e+00,  6.6323e-01,\n",
      "         7.2271e-01, -2.2510e+00,  8.5927e-01, -1.9190e+00, -9.7872e-01,\n",
      "         9.5215e-01,  3.4743e+00,  1.4470e+00,  1.1559e+00, -1.2636e+00,\n",
      "         9.3928e-01,  4.4853e-01, -1.1338e+00, -2.5010e-01, -3.2743e+00,\n",
      "         6.4756e-01,  2.8023e+00,  1.2003e+00, -7.2562e-03,  3.7834e+00,\n",
      "        -8.2738e-01, -2.6711e+00,  1.0068e+00,  9.6686e-01,  1.9361e+00,\n",
      "        -2.0192e+00,  3.4329e+00, -2.0837e+00, -1.5492e-01,  2.3687e-01,\n",
      "        -1.5842e-01, -1.8802e+00,  2.3602e-01, -1.8409e+00,  2.0904e+00,\n",
      "         7.2944e+00,  3.3253e+00, -1.4307e+00,  9.6451e-01,  1.2336e+00,\n",
      "         8.3575e-01,  8.3835e-01,  3.5807e+00,  1.6820e+00, -1.9732e+00,\n",
      "        -6.8178e-01, -3.2648e+00,  3.0617e+00, -9.1528e-01, -4.1637e+00,\n",
      "        -1.2154e+00,  6.4068e-01, -3.8610e+00, -2.7286e-01,  4.1630e-02,\n",
      "        -1.6255e+00, -4.2468e-03, -1.7038e+00, -3.6158e+00, -1.1646e+00,\n",
      "        -5.5467e-01,  4.0077e+00, -3.4334e+00,  1.4676e-01, -1.6042e+00,\n",
      "        -2.8079e+00, -2.7295e+00, -2.9588e+00, -6.0995e+00, -1.9733e-01,\n",
      "        -2.9665e+00, -1.6296e+00,  2.2756e+00, -2.5140e-01,  5.5987e-01,\n",
      "         3.6371e-01,  9.3114e-01, -1.6047e+00,  1.6274e-02,  1.4841e+00,\n",
      "         2.7536e+00, -1.0549e+00,  3.0306e+00, -1.7955e+00,  2.5433e+00,\n",
      "        -3.0371e+00,  5.0226e+00, -1.7849e+00,  8.2026e-01,  1.7467e+00,\n",
      "        -3.2539e-01, -9.2812e-01, -2.8526e+00, -6.8971e-02,  1.3948e+00,\n",
      "         8.4749e-01, -3.3697e+00, -7.2288e-01,  5.8191e-01, -3.9411e+00,\n",
      "         3.0374e+00,  1.3561e+00, -9.7699e-01,  2.8367e+00, -2.2213e+00,\n",
      "         2.2624e+00, -2.9543e+00,  1.9904e+00, -1.9343e+00, -1.4867e+00,\n",
      "         4.0777e+00,  2.0131e+00,  8.5248e-01, -1.1611e+00, -6.5690e-01,\n",
      "         3.0250e-01,  1.5139e+00,  3.9756e+00,  6.1560e-01, -1.5962e+00,\n",
      "        -2.0753e-01,  3.1981e+00, -1.8467e+00,  1.0104e+00,  5.0556e-01,\n",
      "        -1.6913e+00, -2.7009e+00,  1.5037e+00,  2.3164e+00, -1.4218e+00,\n",
      "        -3.0329e+00, -2.5533e+00, -1.5373e+00,  3.0144e+00,  2.4561e+00,\n",
      "         1.4619e-01, -3.0830e-01, -4.0992e-01,  5.7862e-01,  7.7826e-01,\n",
      "         2.9675e-01,  2.1760e+00,  2.3739e-02,  2.7197e+00,  1.8565e+00,\n",
      "        -1.6897e+00, -2.6974e+00, -2.5229e+00,  1.1674e+00, -1.0450e+00,\n",
      "        -1.9003e+00,  1.1350e+00,  1.4166e+00, -1.5674e+00,  2.8417e+00,\n",
      "        -6.5547e-01, -4.8521e-01, -6.6295e-02,  4.9813e-02,  5.3233e+00,\n",
      "        -1.9762e+00,  3.0044e-01, -8.0354e-01, -1.3580e-01,  1.5316e+00,\n",
      "        -3.4931e+00,  1.9646e+00,  1.0261e+00, -1.3227e+00,  4.1494e+00,\n",
      "         2.6737e+00,  1.4811e+00,  2.0949e+00, -1.1744e+00, -9.9300e-01,\n",
      "        -2.8528e+00,  6.2770e-01, -3.0423e-01,  1.1352e+00, -1.0993e-01,\n",
      "         2.3138e+00,  6.4914e-01,  1.1525e+00, -3.4596e+01, -6.3030e-01,\n",
      "         7.0183e-01, -1.1096e+00,  1.2954e+00, -2.4336e-01,  1.8730e-01,\n",
      "        -9.7986e-01, -2.4255e+00, -2.7540e+00,  2.5795e-01,  4.4657e-01,\n",
      "         1.4691e+00, -1.3968e-01,  1.6691e+00, -1.6679e+00, -6.0432e-01,\n",
      "        -1.8309e+00, -7.2813e-01,  6.2587e+00, -4.1484e+00, -1.8954e+00,\n",
      "         1.7129e+00, -1.7392e+00, -1.9080e+00, -3.8944e+00,  2.1198e+00,\n",
      "         3.1510e+00, -1.8239e+00, -3.4875e+00,  6.5917e-01, -2.2250e-01,\n",
      "         4.8521e-01,  1.6412e+00, -1.2759e+00, -1.5504e-01, -3.9104e-01,\n",
      "        -9.7531e-01,  5.0975e+00, -1.3146e+00,  2.5757e-01,  2.7219e+00,\n",
      "         1.2071e+00,  2.5844e+00, -4.3095e-01, -8.0356e-01, -8.2419e-01,\n",
      "        -5.7052e-01,  2.5612e+00, -6.6839e-01, -9.0287e-01, -1.3483e+00,\n",
      "        -1.4342e+00, -1.3068e+00,  2.2311e+00,  1.0935e-01,  1.1391e+00,\n",
      "         1.3930e+00,  1.5698e+00, -7.7640e-01,  2.8144e+00, -2.9195e+00,\n",
      "         1.7380e+00,  7.3721e-01, -9.2170e-02,  1.8021e+00, -5.2267e+00,\n",
      "         4.6592e-01, -2.9761e+00,  1.3978e+00,  1.4958e+00, -3.2461e+00,\n",
      "         1.4359e+00, -1.1351e+01, -1.9313e+00,  5.3672e-01, -8.8799e-02,\n",
      "        -3.1402e+00, -2.7942e-01, -1.0157e+00, -1.0353e+00, -4.4408e+00,\n",
      "         3.3034e-01, -8.3459e-01,  5.3119e-01,  6.3060e-01, -7.3434e-01,\n",
      "        -5.9298e+00, -5.0765e-01,  2.0936e-01,  8.2423e-01,  1.1539e+00,\n",
      "         2.2219e+00,  2.1464e+00,  8.8747e-01,  1.4574e+00,  1.0050e+00,\n",
      "        -9.4127e-01, -6.1036e-01, -7.8086e-01, -3.2449e+00, -5.4982e+00,\n",
      "        -1.9651e-01,  7.7741e-01,  1.4980e+00,  2.8141e+00, -5.9063e+00,\n",
      "         3.3373e+00,  1.4417e+00,  7.6069e-01,  8.6728e-01, -8.9333e-01,\n",
      "         1.0060e-01,  1.4194e+00, -1.6831e+00,  9.2142e-01, -1.4597e+00,\n",
      "         1.3498e+00, -1.0730e+00,  7.4039e-01, -2.1775e+00,  3.6019e-01,\n",
      "        -3.7188e+00, -9.4094e-01, -2.3431e+00, -1.2703e+00, -1.0172e+00,\n",
      "        -3.5447e+00, -3.1287e+00, -2.9816e+00, -2.8667e+00,  9.6980e-01,\n",
      "        -1.2422e-01, -8.9220e-01,  2.3902e+00,  8.3554e-01, -2.2203e+00,\n",
      "         3.4570e+00, -1.1921e+00,  5.8855e-01,  2.1755e+00, -5.1770e-01,\n",
      "         2.0119e+00, -1.6074e+00, -1.4884e+00,  4.3047e-01,  2.5943e+00,\n",
      "        -1.6407e+00, -7.5206e-01,  2.4363e+00,  1.8525e+00, -1.0506e+00,\n",
      "        -1.1457e-01, -7.2980e-01,  2.0110e-02, -1.4724e+00, -1.6587e+00,\n",
      "        -2.4578e+00,  3.7297e+00, -3.8666e-02,  2.2167e+00, -1.1935e+00,\n",
      "         3.5594e+00, -7.5640e-01,  7.4827e-01,  1.8284e+00, -2.9846e+00,\n",
      "         1.7590e+00, -2.9213e+00,  1.2552e-01,  1.0358e+00, -2.9711e+00,\n",
      "        -7.8475e-01,  1.3952e+00,  3.2876e+00, -2.3546e+00,  8.6406e-01,\n",
      "        -2.6333e+00,  1.3878e+00,  4.6163e+00,  2.4878e+00,  1.6297e+00,\n",
      "         1.3551e+00,  4.1563e-01,  7.4159e-01,  3.7548e+00,  3.9805e-01,\n",
      "         1.8668e+00,  1.7023e+00,  1.0502e+00, -1.7296e+00, -1.0199e+00,\n",
      "         1.2966e+00, -1.2876e+00,  9.8251e-01, -2.3306e+00,  9.7415e-01,\n",
      "         3.3839e+00,  2.1420e+00, -5.4117e-01,  1.2401e+00, -1.6248e+00,\n",
      "         6.0959e-01, -3.1886e-01,  6.6971e-01,  2.4183e-01, -1.7406e+00,\n",
      "        -9.8733e-01, -2.4999e-01, -3.8213e-01,  3.1897e-01, -8.5381e-01,\n",
      "        -3.4372e+00, -5.2987e+00,  1.9586e+00,  1.0012e-01,  2.0451e+00,\n",
      "         1.5379e+00, -4.7894e-01, -1.6473e+00, -7.4215e-01, -3.7229e+00,\n",
      "         4.9608e-01, -1.6889e+00,  1.1705e-01,  3.4782e-01, -1.7632e+00,\n",
      "        -5.5889e+00, -1.4528e+00, -9.9352e-01,  5.4367e-01, -3.3648e-01,\n",
      "         2.7179e-01, -9.3141e-01,  1.7503e+00, -4.4935e+00, -2.7272e+00,\n",
      "         7.7184e-01, -1.9179e+00, -2.8384e+00,  3.1839e-02, -1.8471e+00,\n",
      "        -9.9610e-01,  1.8635e-02, -1.7531e+00, -4.2827e-01,  2.3288e+00,\n",
      "         2.3242e+00,  4.6623e+00,  6.1622e+00, -3.9092e+00,  2.7126e+00,\n",
      "        -2.1922e+00, -2.8579e-01,  1.7335e-01, -3.4861e+00,  1.2719e+00,\n",
      "        -2.5704e-01, -6.3506e-01,  2.9681e+00, -5.1881e-01,  3.5841e+00,\n",
      "        -1.7242e+00,  2.8699e+00,  4.5801e-01,  9.1415e-01,  6.4304e-01,\n",
      "         1.2583e+00,  2.9177e+00,  2.2076e+00,  1.5654e+00, -1.8723e+00,\n",
      "        -8.5339e-01,  1.9488e+00, -3.2405e+00, -3.0188e-02,  2.8931e+00,\n",
      "        -2.0998e-02,  4.9188e-01, -1.5602e+00, -6.8160e-01,  5.2934e-01,\n",
      "        -2.2618e-01,  3.7520e-01,  2.7176e+00,  1.7257e+00, -1.5228e+00,\n",
      "        -1.7426e+00, -3.6603e+00, -1.8187e+00, -2.6465e-01,  1.0238e+00,\n",
      "        -2.2566e+00,  1.3386e+00, -1.3316e+00,  2.4140e+00, -2.0545e-01,\n",
      "         2.0489e+00, -2.7285e+00, -2.8030e+00, -2.0398e-01, -1.4178e+00,\n",
      "        -4.6670e-01, -2.2630e+00, -5.5054e-01,  1.3419e+00, -5.0483e-01,\n",
      "        -1.3889e+00,  7.3828e-01,  1.6375e+00, -7.4118e-02,  2.8975e+00,\n",
      "         1.7539e+00,  2.0856e+00, -1.5463e+00, -2.9185e-01,  2.4578e+00,\n",
      "        -3.6246e+00,  1.2159e+00,  4.5754e+00, -1.2489e+00,  5.4321e-01,\n",
      "         1.8469e+00, -2.3264e+00, -6.5454e-01,  1.7432e+00, -2.7882e+00,\n",
      "        -6.0696e-01, -2.6243e+00,  4.5853e+00, -4.0189e-01, -5.9405e-02,\n",
      "         2.8259e+00,  2.5648e+00,  1.3690e+00, -7.6010e-01,  9.0892e-01,\n",
      "        -6.6481e-01, -6.5014e-01,  1.6527e-01,  2.6328e+00,  1.0939e-01,\n",
      "        -2.4905e+00,  3.6010e+00, -4.7411e-01, -3.8473e+00, -8.1167e-02,\n",
      "        -7.7866e-01, -1.1853e+00,  4.4460e+00,  2.2250e+00,  7.2443e-01,\n",
      "         2.6880e-01, -1.2325e+00,  3.6896e-01, -2.9257e-01, -2.0125e-01,\n",
      "         7.3090e-01, -4.3521e-01, -3.0598e-01,  4.9863e+00,  5.3852e-01,\n",
      "         3.0042e-01, -1.1110e+00, -1.4847e+00, -1.2468e-01,  2.1274e-02,\n",
      "         6.2452e-01, -1.3059e+00, -2.0662e+00,  6.9105e-01,  1.0357e+00,\n",
      "         2.6257e-01,  3.7196e+00,  8.2920e-01,  1.1727e-01, -2.1722e+00,\n",
      "         1.7800e+00, -1.8100e+00, -2.1283e+00,  3.5729e+00,  9.1216e-01,\n",
      "        -1.4074e+00,  1.2868e+00,  4.5301e+00, -3.7481e+00, -2.6668e-01,\n",
      "         9.2309e-01, -7.0475e-01,  4.7667e-01, -1.6090e+00,  6.7334e-01,\n",
      "        -3.0713e-01, -7.8066e-01,  4.3853e+00,  3.1689e+00, -1.8918e+00,\n",
      "        -5.4196e-01,  9.7103e-01, -5.1682e-01,  1.4520e+00, -2.0442e+00,\n",
      "         1.1838e-01, -1.4816e+00, -2.7808e+00, -2.6556e-01, -7.7518e-01,\n",
      "        -3.4262e-01, -3.5907e+00, -8.9274e-01,  1.2575e+00,  2.9023e+00,\n",
      "        -1.1513e+00,  1.8616e+00, -6.7390e-01, -1.3803e+00,  4.4629e-01,\n",
      "        -1.5826e+00,  3.2185e+00, -1.5404e+00, -3.1864e+00, -4.4694e-01,\n",
      "        -2.0432e-01, -1.7480e+00,  1.2337e+00, -3.1091e+00, -1.4921e-01,\n",
      "         5.2224e-01, -2.1563e+00,  6.2011e-01, -1.0399e+00,  2.4517e+00,\n",
      "         2.4161e+00,  3.9069e+00, -4.5093e-01,  3.6940e-01, -5.6059e-01,\n",
      "        -2.8303e+00,  2.1327e+00, -2.0417e+00, -2.0594e+00,  3.5811e+00,\n",
      "         9.8356e-01, -8.2150e-01, -1.4013e+00,  1.8661e+00,  4.8269e-01,\n",
      "        -1.0940e-01, -1.0294e+00,  7.8332e-01, -6.3280e-01, -1.5806e+00,\n",
      "        -5.3265e-01, -7.5849e-01, -1.6901e+00,  4.1611e+00,  2.2089e+00,\n",
      "         1.4185e+00, -1.2086e+00, -2.0961e+00]), tensor([ 5.7021e-01,  4.5017e-02, -5.4572e-01,  4.8624e-01, -5.1489e-01,\n",
      "        -6.0299e-01,  4.8430e-01, -1.2347e+00,  9.2832e-01,  2.4624e-01,\n",
      "         2.0503e-01, -3.9752e-01,  3.8051e-01, -4.2910e-01, -5.7856e-01,\n",
      "        -4.4319e-01,  3.8010e-02, -3.6881e-01,  3.2448e-01, -1.6779e-01,\n",
      "         5.3911e-03,  3.9485e-02,  6.0113e-01, -1.8801e-01,  1.3932e-01,\n",
      "        -9.0988e-02, -3.4234e-01,  3.2381e-02, -4.6854e-01, -8.0125e-01,\n",
      "        -7.8148e-01, -6.9760e-01, -1.8781e-01,  6.8404e-01,  6.3251e-02,\n",
      "        -1.7252e-01,  4.6745e-01, -3.4618e-01, -3.7412e-01, -1.2348e-01,\n",
      "        -6.6056e-01,  1.5319e-01, -3.6128e-01,  2.5061e-01,  1.1881e-01,\n",
      "        -4.8399e-01,  3.8293e-01,  4.0397e-01, -2.1416e-02,  7.2903e-01,\n",
      "         8.3430e-02,  3.4268e-01, -1.0815e-01,  6.4630e-01,  1.9819e-01,\n",
      "         4.8672e-02,  5.9729e-01, -4.5485e-01,  2.6199e-01,  2.5694e-01,\n",
      "         1.7493e-01,  3.7034e-01, -2.3505e-01, -3.5049e-01,  5.3519e-01,\n",
      "        -1.1028e-01, -1.0034e-01, -5.3333e-01, -4.9808e-01, -1.5331e-01,\n",
      "        -7.7592e-03, -1.3004e+00,  7.2765e-01,  2.7040e-02,  3.0748e-01,\n",
      "         2.3398e-01, -5.5940e-01,  8.3215e-01,  7.2222e-02,  9.4535e-02,\n",
      "        -2.3318e-01, -8.4584e-02, -1.0344e-02, -2.2689e-01,  4.8320e-01,\n",
      "        -2.2381e-01, -6.3136e-02,  5.5104e-02, -1.9222e-01, -4.0921e-01,\n",
      "         4.3693e-01,  2.8311e-01,  1.4498e-01, -1.3355e-01,  1.6992e-02,\n",
      "         4.6185e-01, -4.6184e-01,  1.9340e-01,  6.3162e-02, -2.0201e-01,\n",
      "         1.5643e-01, -1.1490e-01,  2.3614e-01,  8.7896e-01, -1.4540e-01,\n",
      "        -2.6922e-01,  3.3056e-01,  5.1403e-01,  4.9704e-01,  8.3902e-01,\n",
      "         1.0130e+00,  1.0788e-01,  4.0466e-01, -7.5974e-02, -4.9541e-01,\n",
      "        -5.4679e-01,  3.8710e-01,  2.6618e-01,  4.3568e-01,  4.6312e-02,\n",
      "        -6.2918e-01, -7.0881e-01,  1.1873e-01,  3.4485e+00, -9.9466e-03,\n",
      "         3.6024e-02,  1.8944e-01, -7.8624e-01, -1.3282e-01, -4.9036e-01,\n",
      "        -5.7973e-01,  3.5501e-01,  5.5407e-01,  1.0390e+00, -5.7558e-02,\n",
      "         2.3285e-01, -2.8143e-01,  2.4674e-01, -9.4340e-01,  7.9219e-02,\n",
      "        -8.8629e-02,  9.2445e-01,  6.8275e-01, -1.6462e+00, -2.0415e-01,\n",
      "         6.0524e-01,  6.6978e-01,  2.3591e-01,  5.6752e-01, -3.5107e-01,\n",
      "         7.2300e-01, -4.2916e-01, -4.2347e-01,  3.8794e-02, -3.2529e-01,\n",
      "        -2.5998e-01, -1.1854e-01,  2.5864e-01,  3.6645e-01,  8.7967e-01,\n",
      "         4.2358e-01,  5.5158e-01,  5.8168e-01,  2.7087e-01, -5.2662e-01,\n",
      "         3.2902e-01, -1.3303e+00, -2.6319e-01,  7.5283e-01,  6.6104e-01,\n",
      "        -4.9198e-01, -3.7410e-01, -4.0897e-01,  4.9963e-02, -4.9861e-01,\n",
      "         4.6203e-01,  1.4579e-01, -2.4458e-01, -3.7133e-01, -1.0677e+00,\n",
      "        -1.4428e+01, -5.7737e-01, -3.0456e-02,  2.2976e-03,  1.1606e-01,\n",
      "        -3.3322e-01, -9.3494e-01,  2.6487e-02, -1.8868e-02, -1.3187e+00,\n",
      "         2.5423e-01,  1.1740e-01, -7.8549e-01,  2.7966e-01,  4.2855e-01,\n",
      "        -1.2133e-01, -8.1839e-02, -2.2322e-02, -5.0594e-01,  1.0738e-02,\n",
      "         3.2714e-01, -5.2151e-01, -1.9806e-01,  5.6920e-01, -4.8738e-02,\n",
      "        -1.5808e+00,  4.2682e-01, -3.7130e-01,  1.4282e-01,  2.0050e-01,\n",
      "        -1.1373e+00, -2.0210e-01, -2.4670e-02, -8.7245e-01,  5.2166e-01,\n",
      "        -2.1573e-01,  3.1222e-01, -6.0614e-01, -7.8917e-01, -3.8263e-01,\n",
      "        -7.9095e-01, -8.1743e-01, -1.0402e-01, -3.4692e-01,  7.1954e-01,\n",
      "        -2.7615e+00,  7.1923e-01,  1.2096e+00,  8.4716e-01,  8.7859e-02,\n",
      "        -2.8598e-01,  3.5659e-01,  6.5761e-01,  5.0038e-01,  6.0808e-01,\n",
      "         4.6850e-02,  1.6242e-01,  2.0937e-01, -5.9033e-01, -3.1004e-02,\n",
      "         2.0592e-01,  4.8579e-01,  3.1051e-01, -3.5404e-01, -6.9631e-01,\n",
      "        -1.0317e+00, -2.2518e-01,  8.7741e-01,  9.1413e-01, -6.6901e-02,\n",
      "        -4.3426e-01, -9.6022e-01,  1.3403e-01, -6.0707e-01,  5.3317e-01,\n",
      "         1.0344e+00, -8.5784e-02,  2.3972e-01,  1.0189e+00, -3.0497e-01,\n",
      "         3.3324e-01,  8.3483e-01,  3.0091e-01,  1.1624e-01, -6.5740e-01,\n",
      "         1.1907e+00,  2.3193e-01,  2.6279e-01, -2.9385e-01,  4.0671e-01,\n",
      "         6.9434e-02, -2.7732e-01, -1.1964e-01,  5.7520e-01, -4.2526e-02,\n",
      "        -9.0424e-01,  4.0939e-01, -2.5105e-03,  4.0974e-01,  9.7098e-04,\n",
      "        -3.7746e-01, -1.1640e-01,  3.7766e-02,  7.9603e-01,  5.4743e-01,\n",
      "        -5.6571e-01, -1.1242e+00, -1.1415e-01,  4.3994e-01, -6.8368e-01,\n",
      "        -2.5747e-01, -3.2204e-01,  2.4630e-01,  3.2729e-01, -1.9010e-02,\n",
      "         1.2591e-01,  8.0753e-01,  1.3666e-02, -6.9485e-01, -2.3078e-01,\n",
      "         3.5101e-01, -7.0569e-01, -2.5111e-01, -1.7664e-01, -7.7739e-01,\n",
      "        -2.6079e-01,  3.7808e-01,  2.0593e-01,  5.2402e+00, -2.1338e-01,\n",
      "         5.8317e-02, -7.0727e-01,  5.6196e-02,  2.3775e-01,  2.7805e-02,\n",
      "        -3.1142e-01, -4.4389e-01, -3.0653e-01, -1.4602e-01, -6.7200e-01,\n",
      "         3.7723e-01,  4.3525e-01, -2.9705e-01,  1.0459e+00, -8.2821e-02,\n",
      "        -2.9127e-01, -7.1405e-01, -4.0189e-01,  6.8995e-02,  5.2455e-01,\n",
      "         6.3924e-02, -3.5413e-01,  7.5573e-01, -5.3375e-01, -6.5656e-01,\n",
      "         5.7458e-02,  1.2993e-02, -1.3950e-01, -2.5181e-01, -5.7902e-01,\n",
      "        -3.5626e-01, -7.6575e-01,  7.1585e-02,  4.9139e-01, -1.7781e-01,\n",
      "         5.8261e-01,  5.6295e-01, -1.2801e-01, -1.1191e-02,  9.8803e-01,\n",
      "         9.6015e-03, -8.6575e-01, -1.0149e+00, -1.7558e-01, -4.7420e-01,\n",
      "        -1.7038e-01,  5.4233e-02,  3.2137e-01,  2.0841e-01, -2.1681e-01,\n",
      "        -3.0067e-01,  2.9607e-01, -8.1275e-01, -4.9286e-01, -5.2419e-02,\n",
      "         3.8969e-01,  8.3905e-02, -3.7863e-01,  4.7685e-01, -8.5433e-01,\n",
      "         5.0115e-01,  2.2297e-01, -3.1615e-01,  3.8084e-01,  5.5021e-01,\n",
      "         4.6384e-01, -4.0202e-01,  8.8177e-02, -6.0421e-02, -7.0538e-01,\n",
      "        -8.1491e-01, -9.9396e+00, -7.3863e-01,  4.0132e-01, -7.9975e-01,\n",
      "         3.0316e-01,  7.3720e-01, -2.0690e-01,  3.0287e-01,  4.2738e-03,\n",
      "        -7.1279e-03,  3.3050e-01, -1.1915e-01,  1.0341e-01,  1.1152e-01,\n",
      "        -2.3861e-01,  7.4750e-01, -2.9831e-01,  1.9315e-01,  3.4759e-01,\n",
      "        -1.5379e-02, -1.7352e-01,  4.7801e-02,  6.9428e-01,  6.4196e-01,\n",
      "        -4.7576e-02,  5.0328e-01,  1.9182e-01, -3.4657e-01, -1.0614e-01,\n",
      "         8.2547e-01, -1.0678e-01, -2.7125e-01,  3.9956e-01, -9.0525e-01,\n",
      "        -4.7250e-01, -6.3535e-01, -1.5337e-01,  1.7101e-01, -6.2100e-01,\n",
      "         4.9330e-01, -4.9804e-02, -3.7028e-01, -3.5598e-01, -3.2784e-01,\n",
      "         5.9262e-01,  7.3679e-03,  6.5339e-01,  1.0576e+00,  1.2275e-01,\n",
      "         6.2218e-01,  3.4444e-01, -1.6478e-01, -3.0004e-01,  4.8862e-01,\n",
      "        -7.1154e-01,  1.1675e-01, -1.5468e-01,  2.9611e-01,  3.1893e-01,\n",
      "        -3.8603e-01, -6.0162e-01, -5.6684e-01,  1.0385e+00,  6.8232e-01,\n",
      "         1.5986e+00,  8.8108e-01,  4.5825e-01,  3.0674e-01,  5.4464e-01,\n",
      "        -8.3548e-01, -3.2383e-01, -1.9309e-01,  7.6047e-01, -7.0047e-01,\n",
      "         3.2665e-01,  7.8836e-01,  3.3664e-02,  7.1984e-01,  5.1663e-01,\n",
      "         5.2549e-01, -8.7706e-01,  8.0777e-02, -1.1037e+00, -6.8792e-02,\n",
      "         5.2896e-02, -1.3788e+00,  4.4129e-01, -6.2771e-01,  1.4783e-02,\n",
      "         6.0486e-01, -7.3564e-02, -6.3506e-01, -9.5289e-01,  5.5467e-01,\n",
      "        -4.7058e-01, -3.3460e-01,  3.7530e-01,  6.3587e-01, -1.3049e-01,\n",
      "         7.7427e-01, -2.8169e-01, -8.5597e-02,  5.4974e-01,  2.1065e-01,\n",
      "        -1.0763e-01,  1.1394e-01, -1.9977e-01,  2.7886e-01, -9.3706e-01,\n",
      "         7.6851e-03, -1.3546e-01,  2.1603e-01,  9.6845e-02, -3.9559e-01,\n",
      "        -3.4737e-01,  4.8126e-01, -6.4379e-02,  5.3009e-01, -1.2197e+00,\n",
      "        -9.1150e-01,  4.8683e-04, -1.6615e-01,  7.3206e-01,  4.5861e-01,\n",
      "        -6.7962e-01,  5.1554e-01,  1.6171e-01,  8.0072e-01,  1.6365e-01,\n",
      "        -3.9973e-01, -4.3021e-02, -5.1799e-01,  5.4734e-02,  5.6970e-02,\n",
      "        -3.6896e-01, -3.2560e-01, -5.2340e-01,  3.7775e-01, -5.7113e-01,\n",
      "        -1.7685e-01, -6.2343e-01,  4.0875e-01,  5.6923e-01,  3.3604e-01,\n",
      "        -4.0570e-02,  1.8777e+00, -4.7816e-01, -8.8571e-01, -6.0437e-01,\n",
      "        -1.6873e-01,  4.7100e-01,  2.1168e-01, -1.0963e+00, -1.0060e+00,\n",
      "         2.0612e-01, -6.9168e-01,  2.6592e-01,  2.7639e-01,  1.3405e+00,\n",
      "        -6.9493e-01, -7.0820e-02,  8.7462e-01,  4.0949e-02, -1.8487e-01,\n",
      "         7.2373e-01,  3.1417e-01, -1.7487e-01, -7.5642e-01, -2.7380e-01,\n",
      "         1.5694e-01, -2.2445e-01,  7.6794e-02, -2.8510e-01,  4.2968e-01,\n",
      "         1.5991e+00, -1.0944e-02, -9.6187e-01, -3.2820e-01, -3.3049e-02,\n",
      "         3.3608e-01,  1.4951e-01, -5.6278e-01, -4.5011e-01, -7.4812e-02,\n",
      "        -1.0338e-01, -4.2420e-01, -6.7048e-01,  5.3479e-01,  4.8801e-03,\n",
      "         8.5254e-01, -8.0279e-01,  2.1153e-01,  2.9481e-01,  2.6806e-01,\n",
      "        -2.0365e-01, -2.8144e-01,  5.6003e-01, -1.3689e-01,  6.2487e-01,\n",
      "         1.4695e-01,  7.6032e-02, -8.5575e-01,  3.5972e-01,  3.9613e-01,\n",
      "        -2.3180e-01, -8.6516e-01,  1.6541e-01, -2.0550e-01,  4.7269e-01,\n",
      "        -4.8385e-01, -3.0287e-01, -3.7990e-01,  1.9245e-01,  2.0760e-01,\n",
      "         5.4520e-01, -1.8940e-01, -5.3012e-01,  5.2809e-01, -8.1975e-02,\n",
      "        -5.5708e-01, -3.4166e-02, -4.7881e-01,  1.1630e-01, -5.8904e-01,\n",
      "         6.9264e-01, -5.4412e-01, -5.1858e-01, -2.1759e-02,  8.9122e-01,\n",
      "         5.0927e-02, -1.3046e-01, -1.8333e-01, -1.9169e-01,  7.4103e-01,\n",
      "        -1.4587e-01, -7.7370e-02, -2.3219e-01, -6.7936e-01,  6.5416e-01,\n",
      "         1.1166e-01,  5.4864e-01, -1.4823e+00, -2.3068e-01,  4.2264e-03,\n",
      "        -8.2591e-01, -1.5800e-02,  1.0154e+00,  2.0077e-01, -6.1968e-02,\n",
      "        -1.1889e-01, -7.2409e-01, -7.4437e-02,  3.0595e-03, -6.2827e-01,\n",
      "        -6.6584e-01, -8.9280e-01,  1.9696e-02,  8.4376e-01,  8.6147e-01,\n",
      "         2.1405e-01,  4.6051e-01,  1.5148e-02,  1.3036e-02,  6.0187e-01,\n",
      "         1.1170e+00, -1.3413e-01, -1.2419e+00, -3.3548e-02,  1.3934e-01,\n",
      "        -4.5055e-01,  8.6005e-02, -2.1305e-01, -1.7595e-01,  2.3277e-01,\n",
      "        -5.7171e-02, -2.8437e-01, -4.8373e-01,  2.7903e-01,  1.0918e-01,\n",
      "        -3.6840e-01,  6.2386e-01, -5.9726e-01,  5.1759e-01,  3.1259e-01,\n",
      "         5.7432e-01, -1.2819e-01, -5.3730e-02, -4.0992e-01, -8.8511e-01,\n",
      "        -1.2197e+00, -3.9410e-01, -4.2467e-02,  9.3831e-01,  4.5737e-01,\n",
      "        -4.4013e-02, -9.5386e-01,  1.3234e-01,  5.9766e-01,  7.5703e-01,\n",
      "        -3.4220e-01, -7.1320e-01, -1.8505e-01,  4.8780e-01, -4.8632e-01,\n",
      "         5.4670e-01, -9.1218e-01, -4.5001e-03,  8.6015e-01,  3.5680e-01,\n",
      "        -1.6854e-01,  7.2209e-02,  2.6824e-02, -5.0342e-01, -3.3764e-01,\n",
      "        -3.4553e-01,  2.6038e-01,  1.8682e-01,  2.0395e-01, -8.0035e-01,\n",
      "        -2.3754e-01,  1.9650e-01,  6.9127e-01,  3.4260e-01,  2.9421e-01,\n",
      "         2.6088e-01,  3.2802e-01,  5.5360e-01,  9.8574e-01, -5.5439e-01,\n",
      "        -6.2312e-01,  4.1366e-01, -1.5089e-01, -2.6440e-01,  1.0975e+00,\n",
      "        -4.2352e-01,  8.0882e-01, -6.9699e-01, -8.6520e-01,  5.7735e-02,\n",
      "        -5.4044e+00,  2.2548e-01, -3.2687e-01, -1.8061e-01, -2.1208e-01,\n",
      "         5.7923e-01,  1.6424e-01, -5.1233e-01, -4.4660e-01, -3.4742e-01,\n",
      "         4.4795e-01,  1.5397e-01, -3.0550e-01, -2.8521e-01, -9.1227e-02,\n",
      "         8.8766e-01, -3.2556e-01, -3.1913e-01,  7.7592e-01,  1.4378e-01,\n",
      "         4.1884e-01,  3.5357e-01,  6.5985e-01, -6.3818e-01, -9.2601e-02,\n",
      "        -1.8519e-01, -2.7772e-01, -9.1358e-01, -2.8598e-01,  1.8096e-01,\n",
      "         6.3746e-01,  1.4904e-01,  5.6367e-01, -4.8005e-01,  1.0621e+00,\n",
      "        -3.2012e-01,  8.1133e-04, -3.8102e-01,  1.3934e-01,  7.5129e-02,\n",
      "        -5.3513e-01, -6.1068e-01, -6.3528e-02, -5.1383e-01,  2.7876e-01,\n",
      "         3.3847e-01, -9.9394e-01, -1.6726e-01])]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test sentence\"\n",
    "tokenized_text, tokens_tensor, segments_tensor = bert_text_preparation(text)\n",
    "print(tokenized_text)\n",
    "embeddings  = get_sentence_word_embeddings(tokens_tensor, segments_tensor)\n",
    "print(embeddings)\n",
    "print(len(embeddings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentences):\n",
    "    sentence_tokens = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = bert_tokenizer.tokenize(sentence.lower())\n",
    "        sentence_tokens.append([token.replace(\"##\", \"\") for token in tokenized_sentence])\n",
    "    return sentence_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "sentences =  get_tokens(bbc_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText(sentences, vector_size= 768, window = 5, min_count= 1, workers= 4, epochs = 4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model_outputs/fasttext_embeddings_bbc.ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = model.wv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRClassifier():\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression(verbose=1, max_iter=2)\n",
    "\n",
    "    def train(self, train_X, train_Y, test_X, test_Y):\n",
    "        start_time = datetime.now()\n",
    "        self.model.fit(train_X,train_Y)\n",
    "        end_time = datetime.now()\n",
    "        training_time = (end_time - start_time).total_seconds()\n",
    "        predicitons = self.model.predict(test_X)\n",
    "        test_f1, test_precision, test_recall, test_accuracy = get_metrics(predicitons, test_Y)\n",
    "        print('Testing: Accuracy: {:.3%}, Recall: {:.3%}, Precision: {:.3%}, f1: {:.3%}'.format(test_accuracy,test_recall, test_precision, test_f1))\n",
    "        print('Training time: {:.2f}s'.format(training_time))\n",
    "        return test_f1, test_precision, test_recall, test_accuracy "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_sentence_embeddings(sentence, word_level_combination, sentence_level_combination):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = bert_text_preparation(sentence)\n",
    "    bert_embeddings  = get_sentence_word_embeddings(tokens_tensor, segments_tensor)[1:-1]\n",
    "    fastext_embeddings = [fasttext_model[word] for word in tokenized_text[1:-1]]\n",
    "    word_embeddings = []\n",
    "    for bert_embedding, fastext_embedding in zip(bert_embeddings, fastext_embeddings):\n",
    "        fastext_embedding = torch.from_numpy(fastext_embedding)\n",
    "        if word_level_combination == \"cat\":\n",
    "            result = torch.cat((bert_embedding, fastext_embedding),dim=0) \n",
    "        elif word_level_combination == \"sum\":\n",
    "            result = torch.sum(torch.stack((bert_embedding, fastext_embedding),dim=0),dim=0)\n",
    "        word_embeddings.append(result)\n",
    "\n",
    "    if sentence_level_combination == \"cat\":\n",
    "        sentence_embeddings = torch.cat((word_embeddings[:]) ,dim=0)\n",
    "    elif sentence_level_combination == \"sum\":\n",
    "        sentence_embeddings = torch.sum(torch.stack((word_embeddings[:]),dim=0),dim=0)\n",
    "    return sentence_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sentence_embeddings(sentence, sentence_level_combination, inter_sentence_combination):\n",
    "    tokenized_text, tokens_tensor, segments_tensor = bert_text_preparation(sentence)\n",
    "    bert_embeddings  = get_sentence_word_embeddings(tokens_tensor, segments_tensor)[1:-1]\n",
    "    fastext_embeddings = [fasttext_model[word] for word in tokenized_text[1:-1]]\n",
    "    fastext_embeddings = [torch.from_numpy(x) for x in fastext_embeddings]\n",
    "    if sentence_level_combination == \"cat\":\n",
    "        bert_sentence_embeddings = torch.cat((bert_embeddings[:]) ,dim=0)\n",
    "        fasttext_sentence_embeddings = torch.cat((fastext_embeddings[:]) ,dim=0)\n",
    "    elif sentence_level_combination == \"sum\":\n",
    "        bert_sentence_embeddings = torch.sum(torch.stack((bert_embeddings[:]),dim=0),dim=0)\n",
    "        fasttext_sentence_embeddings = torch.sum(torch.stack((fastext_embeddings[:]),dim=0),dim=0)\n",
    "\n",
    "    if inter_sentence_combination == \"cat\":\n",
    "        sentence_embeddings = torch.cat((bert_sentence_embeddings, fasttext_sentence_embeddings) ,dim=0)\n",
    "    elif inter_sentence_combination == \"sum\":\n",
    "        sentence_embeddings = torch.sum(torch.stack((bert_sentence_embeddings, fasttext_sentence_embeddings),dim=0),dim=0)\n",
    "\n",
    "    return sentence_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "<class 'numpy.ndarray'>\n",
      "-3.939239\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test sentence\"\n",
    "sentence_embeddings = combine_sentence_embeddings(text, sentence_level_combination = \"sum\", inter_sentence_combination= \"cat\")\n",
    "print(len(sentence_embeddings))\n",
    "print(type(sentence_embeddings))\n",
    "print(sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this is a test sentence\"\n",
    "sentence_embeddings = get_combined_sentence_embeddings(text, word_level_combination = \"sum\", sentence_level_combination= \"cat\")\n",
    "len(sentence_embeddings)\n",
    "type(sentence_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BBC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "word_level_combination = \"cat\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(bbc_df, bbc_df[\"category\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.6849393964993584, 0.6875924282756581, 0.617622276413093, 0.6781569069069069, 0.6152517289178456]\n",
      "Precision avg: 0.6567 (+/- 0.0739)\n",
      "Recall values: [0.6012301587301587, 0.601656314699793, 0.5572657817564439, 0.5942920251104395, 0.5825491016919588]\n",
      "Recall avg: 0.5874 (+/- 0.0371)\n",
      "F1 values: [0.6390857768790023, 0.6337924158558806, 0.5683749534193782, 0.6151859981212778, 0.5759507952340742]\n",
      "F1 avg: 0.6065 (+/- 0.0653)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.35634D+06\n",
      "Testing: Accuracy: 65.471%, Recall: 54.417%, Precision: 66.421%, f1: 58.360%\n",
      "Training time: 0.17s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.288D+06   2.621D+03\n",
      "  F =   2621.1324985046263     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.37872D+06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.466D+06   2.595D+03\n",
      "  F =   2595.4732407946008     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 69.058%, Recall: 56.788%, Precision: 67.088%, f1: 60.641%\n",
      "Training time: 0.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.41559D+06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.424D+06   2.601D+03\n",
      "  F =   2601.4762697074398     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 65.919%, Recall: 55.051%, Precision: 65.927%, f1: 59.179%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.30226D+06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.340D+06   2.620D+03\n",
      "  F =   2619.6573304305903     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 70.852%, Recall: 59.445%, Precision: 70.707%, f1: 63.587%\n",
      "Training time: 0.03s\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.27041D+06\n",
      "Testing: Accuracy: 71.300%, Recall: 58.019%, Precision: 69.050%, f1: 62.746%\n",
      "Training time: 0.02s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.474D+06   2.597D+03\n",
      "  F =   2597.4978578649170     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "word_level_combination = \"sum\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(bbc_df, bbc_df[\"category\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.6642079423349438, 0.6708779061474826, 0.6592733367789358, 0.7070713505267281, 0.6905040841305923]\n",
      "Precision avg: 0.6784 (+/- 0.0399)\n",
      "Recall values: [0.5441729512317748, 0.5678756674294432, 0.5505064440224078, 0.5944536529389264, 0.5801923456952769]\n",
      "Recall avg: 0.5674 (+/- 0.0415)\n",
      "F1 values: [0.5836032388663968, 0.6064050444732046, 0.5917886897701867, 0.6358734960697848, 0.6274595263984535]\n",
      "F1 avg: 0.6090 (+/- 0.0448)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.36123D+06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.442D+06   2.602D+03\n",
      "  F =   2602.3579899918996     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 72.197%, Recall: 57.666%, Precision: 67.950%, f1: 62.180%\n",
      "Training time: 0.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.36513D+06\n",
      "Testing: Accuracy: 75.336%, Recall: 61.707%, Precision: 70.440%, f1: 65.227%\n",
      "Training time: 0.02s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      7      1     0     0   1.872D+06   2.628D+03\n",
      "  F =   2627.7401174062511     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.23741D+06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      7      1     0     0   2.198D+06   2.606D+03\n",
      "  F =   2605.9363198215183     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 65.919%, Recall: 62.606%, Precision: 59.910%, f1: 56.072%\n",
      "Training time: 0.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.07204D+06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.911D+06   2.519D+03\n",
      "  F =   2519.1130292136709     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 73.543%, Recall: 63.583%, Precision: 65.959%, f1: 63.222%\n",
      "Training time: 0.04s\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3845     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.86480D+03    |proj g|=  1.27400D+06\n",
      "Testing: Accuracy: 65.471%, Recall: 54.987%, Precision: 61.761%, f1: 56.881%\n",
      "Training time: 0.05s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3845      2      8      1     0     0   1.644D+06   2.568D+03\n",
      "  F =   2567.9273184907747     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "inter_sentence_combination = \"sum\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(bbc_df, bbc_df[\"category\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [combine_sentence_embeddings(x, sentence_level_combination, inter_sentence_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [combine_sentence_embeddings(x, sentence_level_combination, inter_sentence_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.6795019553715207, 0.7044032813839184, 0.5990950226244344, 0.6595875112089346, 0.6176051820119617]\n",
      "Precision avg: 0.6520 (+/- 0.0868)\n",
      "Recall values: [0.576656360588564, 0.6170673421045636, 0.6260606060606061, 0.635827067669173, 0.5498681096758122]\n",
      "Recall avg: 0.6011 (+/- 0.0728)\n",
      "F1 values: [0.6218024494727468, 0.6522721794332885, 0.560715569938871, 0.6322174461287087, 0.5688081786795453]\n",
      "F1 avg: 0.6072 (+/- 0.0807)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Movies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.04385D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   2.511D+06   2.432D+03\n",
      "  F =   2431.7653500072593     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 66.200%, Recall: 75.068%, Precision: 64.952%, f1: 61.841%\n",
      "Training time: 0.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  9.99521D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   1.513D+06   2.364D+03\n",
      "  F =   2363.6313772374606     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 65.000%, Recall: 70.026%, Precision: 65.099%, f1: 62.746%\n",
      "Training time: 0.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.01797D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   2.317D+06   2.542D+03\n",
      "  F =   2542.0750166572511     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 63.000%, Recall: 75.852%, Precision: 59.226%, f1: 53.605%\n",
      "Training time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "Testing: Accuracy: 65.000%, Recall: 74.749%, Precision: 64.489%, f1: 60.723%\n",
      "Training time: 0.02s\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.29531D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   2.057D+06   2.653D+03\n",
      "  F =   2653.1261635736259     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.49197D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   1.672D+06   2.686D+03\n",
      "  F =   2686.4417906789945     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 48.200%, Recall: 74.048%, Precision: 50.192%, f1: 32.859%\n",
      "Training time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "word_level_combination = \"sum\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(movies_df, movies_df[\"sentiment\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.6495192307692308, 0.6509944159106545, 0.5922582520509011, 0.6448850905831893, 0.5019230769230769]\n",
      "Precision avg: 0.6079 (+/- 0.1282)\n",
      "Recall values: [0.7506784169375856, 0.700258891435362, 0.7585184849993212, 0.7474917302277263, 0.7404809619238477]\n",
      "Recall avg: 0.7395 (+/- 0.0457)\n",
      "F1 values: [0.6184050686645082, 0.6274550710815725, 0.5360501567398119, 0.6072270227808327, 0.32859461112925714]\n",
      "F1 avg: 0.5435 (+/- 0.2509)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  9.79442D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      6      1     0     0   2.124D+06   2.628D+03\n",
      "  F =   2627.6467544104758     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 52.600%, Recall: 75.000%, Precision: 54.943%, f1: 42.330%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.07893D+05\n",
      "Testing: Accuracy: 75.400%, Recall: 77.236%, Precision: 75.452%, f1: 75.003%\n",
      "Training time: 0.05s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      7      1     0     0   2.576D+05   2.202D+03\n",
      "  F =   2202.2136232423345     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.61941D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      6      1     0     0   1.673D+06   2.693D+03\n",
      "  F =   2692.8504385550414     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 51.600%, Recall: 72.602%, Precision: 56.429%, f1: 44.019%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.14664D+05\n",
      "Testing: Accuracy: 71.000%, Recall: 72.793%, Precision: 70.642%, f1: 70.178%\n",
      "Training time: 0.04s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      7      1     0     0   8.923D+05   2.382D+03\n",
      "  F =   2381.9890035424933     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.11877D+05\n",
      "Testing: Accuracy: 73.800%, Recall: 75.204%, Precision: 73.752%, f1: 73.404%\n",
      "Training time: 0.03s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      8      1     0     0   3.209D+05   2.210D+03\n",
      "  F =   2210.3468688876756     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "word_level_combination = \"cat\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(movies_df, movies_df[\"sentiment\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.5494296577946768, 0.7545160722571561, 0.5642945103376759, 0.7064228995901639, 0.7375238003808061]\n",
      "Precision avg: 0.6624 (+/- 0.1961)\n",
      "Recall values: [0.75, 0.7723614820902678, 0.7260195944406471, 0.7279259691361298, 0.752037351443124]\n",
      "Recall avg: 0.7457 (+/- 0.0384)\n",
      "F1 values: [0.42329873125720874, 0.7500315000955163, 0.4401880227996151, 0.7017823134288452, 0.734041473288425]\n",
      "F1 avg: 0.6099 (+/- 0.3273)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.00235D+05\n",
      "Testing: Accuracy: 65.800%, Recall: 75.202%, Precision: 64.385%, f1: 61.026%\n",
      "Training time: 0.09s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   2.288D+06   2.435D+03\n",
      "  F =   2434.7074694471848     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  9.83710D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   2.036D+06   2.628D+03\n",
      "  F =   2628.4925802333491     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 58.400%, Recall: 76.294%, Precision: 57.213%, f1: 48.413%\n",
      "Training time: 0.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.35635D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   1.786D+06   2.679D+03\n",
      "  F =   2679.1956670379268     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 52.800%, Recall: 76.210%, Precision: 50.833%, f1: 36.031%\n",
      "Training time: 0.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  1.11423D+05\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   2.148D+06   2.478D+03\n",
      "  F =   2477.9899760344770     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 74.200%, Recall: 74.359%, Precision: 74.106%, f1: 74.100%\n",
      "Training time: 0.06s\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+03    |proj g|=  2.56754D+05\n",
      "Testing: Accuracy: 53.000%, Recall: 66.230%, Precision: 56.192%, f1: 46.266%\n",
      "Training time: 0.03s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   6.403D+05   2.657D+03\n",
      "  F =   2656.9711033787794     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "inter_sentence_combination = \"sum\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(movies_df, movies_df[\"sentiment\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [combine_sentence_embeddings(x, sentence_level_combination, inter_sentence_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [combine_sentence_embeddings(x, sentence_level_combination, inter_sentence_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.6438464867984418, 0.5721285487822453, 0.5083333333333333, 0.7410564225690276, 0.5619162640901771]\n",
      "Precision avg: 0.6055 (+/- 0.1797)\n",
      "Recall values: [0.7520222446916077, 0.7629443698558169, 0.7620967741935484, 0.7435947204968945, 0.6622979190409861]\n",
      "Recall avg: 0.7366 (+/- 0.0846)\n",
      "F1 values: [0.6102564102564103, 0.48412698412698413, 0.36030878653829473, 0.7410044209943021, 0.46266308746930324]\n",
      "F1 avg: 0.5317 (+/- 0.2939)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.00302D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   5.677D+03   2.321D+03\n",
      "  F =   2320.6847934734442     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 64.375%, Recall: 64.392%, Precision: 64.411%, f1: 64.367%\n",
      "Training time: 0.04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  9.83035D+03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   4.855D+03   2.337D+03\n",
      "  F =   2336.7772247535427     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 68.542%, Recall: 68.565%, Precision: 68.533%, f1: 68.525%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.60719D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   1.167D+05   2.526D+03\n",
      "  F =   2526.0510596905860     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 58.958%, Recall: 64.745%, Precision: 59.214%, f1: 54.837%\n",
      "Training time: 0.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.02043D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   8.623D+04   2.376D+03\n",
      "  F =   2375.7083622911077     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 67.500%, Recall: 68.765%, Precision: 67.553%, f1: 66.984%\n",
      "Training time: 0.06s\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  9.66439D+03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   9.854D+03   2.338D+03\n",
      "  F =   2338.3452713231777     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 67.500%, Recall: 67.579%, Precision: 67.570%, f1: 67.499%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "word_level_combination = \"sum\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(twitter_df, twitter_df[\"label\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.6441089031450478, 0.6853330786992831, 0.5921418154038475, 0.6755325613291898, 0.675696338899411]\n",
      "Precision avg: 0.6546 (+/- 0.0764)\n",
      "Recall values: [0.6439162441836239, 0.6856521739130435, 0.6474535300491789, 0.6876519608752947, 0.6757940854326396]\n",
      "Recall avg: 0.6681 (+/- 0.0420)\n",
      "F1 values: [0.6436742189094035, 0.6852513689915276, 0.5483701492537313, 0.6698412698412698, 0.6749943575409296]\n",
      "F1 avg: 0.6444 (+/- 0.1117)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.03291D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      7      1     0     0   1.014D+04   2.319D+03\n",
      "  F =   2319.2513221936870     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 69.167%, Recall: 69.178%, Precision: 69.141%, f1: 69.140%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.01292D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      6      1     0     0   4.711D+04   2.325D+03\n",
      "  F =   2325.4043589732933     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 70.208%, Recall: 70.213%, Precision: 70.116%, f1: 70.127%\n",
      "Training time: 0.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.04949D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      6      1     0     0   1.121D+05   2.465D+03\n",
      "  F =   2465.3828619820811     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 63.542%, Recall: 68.340%, Precision: 64.954%, f1: 62.244%\n",
      "Training time: 0.05s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.00288D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      6      1     0     0   1.833D+04   2.322D+03\n",
      "  F =   2322.3345779912074     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 61.667%, Recall: 61.681%, Precision: 61.624%, f1: 61.600%\n",
      "Training time: 0.04s\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         1537     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  9.90653D+03\n",
      "Testing: Accuracy: 68.125%, Recall: 68.301%, Precision: 68.249%, f1: 68.118%\n",
      "Training time: 0.03s\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 1537      2      7      1     0     0   1.741D+04   2.337D+03\n",
      "  F =   2336.9764646873041     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "word_level_combination = \"cat\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(twitter_df, twitter_df[\"label\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [get_combined_sentence_embeddings(x, word_level_combination, sentence_level_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.691410116163984, 0.7011607096314574, 0.6495424836601307, 0.6162421211647653, 0.6824944382647387]\n",
      "Precision avg: 0.6682 (+/- 0.0698)\n",
      "Recall values: [0.6917830859966249, 0.702126545149801, 0.6833960328317373, 0.6168100364676938, 0.6830065359477124]\n",
      "Recall avg: 0.6754 (+/- 0.0674)\n",
      "F1 values: [0.6914041458879949, 0.7012729844413013, 0.6224362748843711, 0.6160000000000001, 0.6811821958663083]\n",
      "F1 avg: 0.6625 (+/- 0.0803)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.02370D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   8.597D+03   2.333D+03\n",
      "  F =   2332.7650694024878     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 72.917%, Recall: 72.974%, Precision: 72.927%, f1: 72.905%\n",
      "Training time: 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  9.32068D+03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   6.475D+03   2.346D+03\n",
      "  F =   2345.7067247537211     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 71.667%, Recall: 71.687%, Precision: 71.687%, f1: 71.667%\n",
      "Training time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.02227D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   9.279D+04   2.344D+03\n",
      "  F =   2344.3330772948498     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 66.250%, Recall: 67.325%, Precision: 66.487%, f1: 65.909%\n",
      "Training time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  9.98811D+03\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      6      1     0     0   7.996D+04   2.382D+03\n",
      "  F =   2382.1923274032656     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 65.417%, Recall: 65.963%, Precision: 65.614%, f1: 65.281%\n",
      "Training time: 0.02s\n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =          769     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.66169D+03    |proj g|=  1.02810D+04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "  769      2      7      1     0     0   6.151D+03   2.330D+03\n",
      "  F =   2330.0432027746488     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "Testing: Accuracy: 69.583%, Recall: 69.592%, Precision: 69.596%, f1: 69.583%\n",
      "Training time: 0.02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/Users/rayenebech/Desktop/rayene/ytu/spring_2022/Hesaplamali Anabilim/HW/HW_1/my-venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "f1s = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "inter_sentence_combination = \"sum\"\n",
    "sentence_level_combination = \"sum\"\n",
    "for i in range(5):\n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(twitter_df, twitter_df[\"label\"], test_size = TRAIN_VAL_SIZE)\n",
    "    val_X, test_X, val_Y, test_Y = train_test_split(val_X, val_Y, test_size = VAL_TEST_SIZE)\n",
    "    train_embeddings = [combine_sentence_embeddings(x, sentence_level_combination, inter_sentence_combination) for x in train_X[\"text\"]]\n",
    "    test_embeddings = [combine_sentence_embeddings(x, sentence_level_combination, inter_sentence_combination) for x in test_X[\"text\"]]\n",
    "    model = LRClassifier()\n",
    "    test_f1, test_precision, test_recall, test_accuracy  = model.train(train_embeddings, train_Y, test_embeddings, test_Y)\n",
    "    f1s.append(test_f1)\n",
    "    recalls.append(test_recall)\n",
    "    precisions.append(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision values: [0.7292661330925885, 0.7168657960544595, 0.6648719062092923, 0.6561392537002293, 0.6959572103362045]\n",
      "Precision avg: 0.6926 (+/- 0.0636)\n",
      "Recall values: [0.7297447763531029, 0.7168657960544595, 0.673249256264715, 0.6596334185848252, 0.6959163830821585]\n",
      "Recall avg: 0.6951 (+/- 0.0584)\n",
      "F1 values: [0.7290490664350846, 0.7166666666666666, 0.6590909090909092, 0.6528104575163398, 0.6958280525703572]\n",
      "F1 avg: 0.6907 (+/- 0.0679)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision values:\", precisions)\n",
    "print(\"Precision avg: %0.4f (+/- %0.4f)\" % (statistics.mean(precisions), statistics.stdev(precisions) * 2))\n",
    "print(\"Recall values:\", recalls)\n",
    "print(\"Recall avg: %0.4f (+/- %0.4f)\" % (statistics.mean(recalls), statistics.stdev(recalls) * 2))\n",
    "print(\"F1 values:\", f1s)\n",
    "print(\"F1 avg: %0.4f (+/- %0.4f)\" % (statistics.mean(f1s), statistics.stdev(f1s) * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3dd4a2e0defd57de5fbee3712486c1f066f8e1e168ee0000614ffba8faf61667"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
